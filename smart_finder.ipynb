{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954d2fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß VAULT SETUP - One-time configuration\n",
      "==================================================\n",
      "‚úÖ Found 227 markdown files\n",
      "‚úÖ Vault configured and saved!\n",
      "Vault ready at: D:\\LOST.DIR\\Obsidian Vault\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class VaultConfig:\n",
    "    def __init__(self):\n",
    "        self.config_file = Path(\"vault_config.json\")\n",
    "        \n",
    "    def setup_vault(self):\n",
    "        \"\"\"One-time vault setup\"\"\"\n",
    "        print(\"üîß VAULT SETUP - One-time configuration\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if self.config_file.exists():\n",
    "            with open(self.config_file, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                print(f\"üìÅ Current vault: {config['vault_path']}\")\n",
    "                \n",
    "                change = input(\"Change vault path? (y/n): \").strip().lower()\n",
    "                if not change.startswith('y'):\n",
    "                    return config['vault_path']\n",
    "        \n",
    "        while True:\n",
    "            vault_path = input(\"\\nEnter your vault path: \").strip()\n",
    "            \n",
    "            if not vault_path:\n",
    "                print(\"‚ùå Please provide a path\")\n",
    "                continue\n",
    "                \n",
    "            if not Path(vault_path).exists():\n",
    "                print(f\"‚ùå Path {vault_path} doesn't exist\")\n",
    "                continue\n",
    "                \n",
    "            # Test for .md files\n",
    "            md_files = list(Path(vault_path).rglob(\"*.md\"))\n",
    "            if not md_files:\n",
    "                print(f\"‚ùå No .md files found in {vault_path}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"‚úÖ Found {len(md_files)} markdown files\")\n",
    "            break\n",
    "        \n",
    "        # Save configuration\n",
    "        config = {\"vault_path\": vault_path}\n",
    "        with open(self.config_file, 'w') as f:\n",
    "            json.dump(config, f)\n",
    "            \n",
    "        print(f\"‚úÖ Vault configured and saved!\")\n",
    "        return vault_path\n",
    "    \n",
    "    def get_vault_path(self):\n",
    "        \"\"\"Get configured vault path\"\"\"\n",
    "        if not self.config_file.exists():\n",
    "            return self.setup_vault()\n",
    "            \n",
    "        with open(self.config_file, 'r') as f:\n",
    "            config = json.load(f)\n",
    "            return config['vault_path']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = VaultConfig()\n",
    "    vault_path = config.setup_vault()\n",
    "    print(f\"Vault ready at: {vault_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58303861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading vault: D:\\LOST.DIR\\Obsidian Vault\n",
      "üì• Loading 227 markdown files...\n",
      "   Loaded 100/227 files...\n",
      "   Loaded 200/227 files...\n",
      "‚úÖ Loaded 227 files successfully\n",
      "üîß Building vocabulary for spell checking...\n",
      "‚úÖ Built vocabulary with 2984 words\n",
      "ü§ñ Loading AI model for semantic search...\n",
      "\n",
      "================================================================================\n",
      "ü§ñ SMART DOCUMENT FINDER\n",
      "‚ú® Features: Auto keyword/semantic search ‚Ä¢ Multi-select ‚Ä¢ Spell check\n",
      "üìÅ Vault: 227 documents loaded\n",
      "üß† AI semantic search: ENABLED\n",
      "================================================================================\n",
      "üéØ Using SEMANTIC search (14 words)\n",
      "üß† Semantic search for: 'model context protocol is a smart way to add external database to your llm'\n",
      "   Processing document chunks...\n",
      "   Encoding 487 chunks with AI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:34<00:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã FOUND 10 RELEVANT DOCUMENTS:\n",
      "================================================================================\n",
      "\n",
      "1. 1. Intro.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\4. MCP\n",
      "   üìè Size: 1,756 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.541\n",
      "   üìù Preview:\n",
      "      ---\n",
      "      title: 1. Intro\n",
      "      updated: 2025-07-15 06:10:31Z\n",
      "\n",
      "2. Intro.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\MCP\n",
      "   üìè Size: 1,865 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.526\n",
      "   üìù Preview:\n",
      "      ---\n",
      "      title: Intro\n",
      "      updated: 2025-07-26 16:49:21Z\n",
      "\n",
      "3. Things left to do in RAG.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\11. RAG\n",
      "   üìè Size: 3,629 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.402\n",
      "   üìù Preview:\n",
      "      This is similar to **RAG-Fusion**, but with **model-driven feedback loops**.\n",
      "      \n",
      "      ### Key Ideas:\n",
      "\n",
      "4. Revision 15-7-25.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\4. MCP\n",
      "   üìè Size: 2,140 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.390\n",
      "   üìù Preview:\n",
      "      ---\n",
      "      title: Revision 15-7-25\n",
      "      updated: 2025-07-15 07:32:41Z\n",
      "\n",
      "5. In-Context Learning, how it works.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\19. Summary Of Research Paper\n",
      "   üìè Size: 1,763 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.386\n",
      "   üìù Preview:\n",
      "      ---\n",
      "      title: In-Context Learning, how it works\n",
      "      updated: 2025-07-26 10:19:32Z\n",
      "\n",
      "6. Query Routing.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\15. Holiday Studying GenAI\\Articles on RAG\n",
      "   üìè Size: 14,665 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.351\n",
      "   üìù Preview:\n",
      "      Also,¬†**LLM‚Äôs output may not be consistent**. Although LLM can route ambiguous queries more effectiv...\n",
      "      \n",
      "      ## Semantic query routing\n",
      "\n",
      "7. Discrimination Vs Generation.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\12. Transformers\n",
      "   üìè Size: 23,075 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.347\n",
      "   üìù Preview:\n",
      "      1. **Generative Nature of LLMs**:\n",
      "          \n",
      "          - LLMs like GPT-4 generate text based on a given prompt by predicting the next word (or token) i...\n",
      "\n",
      "8. DataBase.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\20. Stuff-I-Don_t-Need\\3 - CAMPUS_X\\SQL\n",
      "   üìè Size: 3,385 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.334\n",
      "   üìù Preview:\n",
      "      ---\n",
      "      title: DataBase\n",
      "      updated: 2024-09-28 12:19:09Z\n",
      "\n",
      "9. Hypothetical Questioning.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\11. RAG\n",
      "   üìè Size: 8,016 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.326\n",
      "   üìù Preview:\n",
      "      That‚Äôs why you don‚Äôt let the LLM hallucinate ‚Äî you force it to **retrieve supporting documents**, th...\n",
      "      \n",
      "      ---\n",
      "\n",
      "10. 18th May.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\2. Chrome\\14. Tabs to Read (Before Exams)\n",
      "   üìè Size: 1,440 chars\n",
      "   üîç Search: SEMANTIC\n",
      "   üìà Relevance: 0.318\n",
      "   üìù Preview:\n",
      "      ---\n",
      "      title: 18th May\n",
      "      updated: 2025-05-18 05:29:23Z\n",
      "\n",
      "üéØ SELECT DOCUMENTS (Multiple Selection Supported):\n",
      "Examples: 1,3,5 | 1-3 | all | quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö VIEWING 2 SELECTED DOCUMENTS:\n",
      "====================================================================================================\n",
      "\n",
      "üìñ DOCUMENT 1/2: 1. Intro.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\4. MCP\\1. Intro.md\n",
      "üîç Found via: SEMANTIC search\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: 1. Intro\n",
      "updated: 2025-07-15 06:10:31Z\n",
      "created: 2025-07-14 07:49:58Z\n",
      "---\n",
      "\n",
      "MCP is a framework through which developers can embedded context and meaning to the model without explicitly mentioning what tool and which model to use. The MCP server takes care of that. You just declare what needs to happen instead of coding manually.\n",
      "\n",
      "MCP (Model Context Protocol) is a framework that lets developers describe what context (like tools, documents, memory, or functions) a model should use‚Äîwithout manually coding how to connect them. Instead of wiring everything together, you just declare what you need, and the system sets it up so the model can use it intelligently.\n",
      "\n",
      "MCP lets you describe what a model should have access to‚Äîlike tools or memory‚Äîwithout writing all the code to wire it up. The platform handles that for you.\n",
      "\n",
      "Model Context Protocol (MCP) is an open standard developed by Anthropic, the company behind Claude. While it may sound technical, but the core idea is simple:¬†give AI agents a consistent way to connect with tools, services, and data ‚Äî no matter where they live or how they‚Äôre built.\n",
      "\n",
      "![[Pasted image 20250714150825.png]]\n",
      "\n",
      "Client - is what allows us to access some functionality which is present inside the MCP server\n",
      "Resources in MCP allow your server to expose information that can be directly included in prompts, rather than requiring tool calls to access data. This creates a more efficient way to provide context to AI models.\n",
      "\n",
      "![[Pasted image 20250714163303.png]]\n",
      "\n",
      "## Choosing the Right Primitive\n",
      "\n",
      "Here's a quick decision guide:\n",
      "\n",
      "- **Need to give Claude new capabilities?**¬†Use tools\n",
      "- **Need to get data into your app for UI or context?**¬†Use resources\n",
      "- **Want to create predefined workflows for users?**¬†Use prompt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ DOCUMENT 2/2: Intro.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\MCP\\Intro.md\n",
      "üîç Found via: SEMANTIC search\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: Intro\n",
      "updated: 2025-07-26 16:49:21Z\n",
      "created: 2025-07-26 16:38:57Z\n",
      "---\n",
      "\n",
      "The AI ecosystem is evolving rapidly, with Large Language Models (LLMs) and other AI systems becoming increasingly capable. However, these models are often limited by their training data and lack access to real-time information or specialized tools. This limitation hinders the potential of AI systems to provide truly relevant, accurate, and helpful responses in many scenarios.\n",
      "\n",
      "This is where Model Context Protocol (MCP) comes in. MCP enables AI models to connect with external data sources, tools, and environments, allowing for the seamless transfer of information and capabilities between AI systems and the broader digital world\n",
      "\n",
      "¬†**M√óN Integration Problem**\n",
      "- **Host**: The user-facing AI application that end-users interact with directly. Examples include Anthropic‚Äôs Claude Desktop, AI-enhanced IDEs like Cursor, inference libraries like Hugging Face Python SDK, or custom applications built in libraries like LangChain or smolagents. Hosts initiate connections to MCP Servers and orchestrate the overall flow between user requests, LLM processing, and external tools.\n",
      "    \n",
      "- **Client**: A component within the host application that manages communication with a specific MCP Server. Each Client maintains a 1:1 connection with a single Server, handling the protocol-level details of MCP communication and acting as an intermediary between the Host‚Äôs logic and the external Server.\n",
      "    \n",
      "- **Server**: An external program or service that exposes capabilities (Tools, Resources, Prompts) via the MCP protocol.\n",
      "## Resources\n",
      "\n",
      "Resources provide read-only access to data sources, allowing the AI model to retrieve context without executing complex logic.\n",
      "\n",
      "## Sampling\n",
      "\n",
      "Sampling allows Servers to request the Client (specifically, the Host application) to perform LLM interactions.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "ü§ñ SMART DOCUMENT FINDER\n",
      "‚ú® Features: Auto keyword/semantic search ‚Ä¢ Multi-select ‚Ä¢ Spell check\n",
      "üìÅ Vault: 227 documents loaded\n",
      "üß† AI semantic search: ENABLED\n",
      "================================================================================\n",
      "‚ùå Please enter a query\n",
      "\n",
      "================================================================================\n",
      "ü§ñ SMART DOCUMENT FINDER\n",
      "‚ú® Features: Auto keyword/semantic search ‚Ä¢ Multi-select ‚Ä¢ Spell check\n",
      "üìÅ Vault: 227 documents loaded\n",
      "üß† AI semantic search: ENABLED\n",
      "================================================================================\n",
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import difflib\n",
    "from typing import List, Dict\n",
    "import sys\n",
    "\n",
    "# Try to import sentence-transformers, fallback if not available\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "    SEMANTIC_SEARCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SEMANTIC_SEARCH_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  sentence-transformers not installed. Semantic search disabled.\")\n",
    "    print(\"   Install with: pip install sentence-transformers\")\n",
    "\n",
    "from vault_config import VaultConfig\n",
    "\n",
    "class SmartDocumentFinder:\n",
    "    def __init__(self):\n",
    "        # Get vault path from config\n",
    "        self.config = VaultConfig()\n",
    "        self.vault_path = Path(self.config.get_vault_path())\n",
    "        \n",
    "        print(f\"üìÅ Loading vault: {self.vault_path}\")\n",
    "        \n",
    "        # Load all files once\n",
    "        self.md_files = list(self.vault_path.rglob(\"*.md\"))\n",
    "        self.file_contents = {}\n",
    "        self.common_words = set()\n",
    "        \n",
    "        self._load_all_files()\n",
    "        self._build_vocabulary()\n",
    "        \n",
    "        # Initialize semantic search if available\n",
    "        if SEMANTIC_SEARCH_AVAILABLE:\n",
    "            print(\"ü§ñ Loading AI model for semantic search...\")\n",
    "            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            self.chunks_cache = {}  # Cache chunks to avoid reprocessing\n",
    "        else:\n",
    "            self.model = None\n",
    "    \n",
    "    def _load_all_files(self):\n",
    "        \"\"\"Load all markdown files into memory\"\"\"\n",
    "        print(f\"üì• Loading {len(self.md_files)} markdown files...\")\n",
    "        \n",
    "        processed = 0\n",
    "        for file_path in self.md_files:\n",
    "            processed += 1\n",
    "            if processed % 100 == 0:\n",
    "                print(f\"   Loaded {processed}/{len(self.md_files)} files...\")\n",
    "                \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    if len(content.strip()) > 50:  # Skip very small files\n",
    "                        self.file_contents[str(file_path)] = content\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error loading {file_path}: {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(self.file_contents)} files successfully\")\n",
    "    \n",
    "    def _build_vocabulary(self):\n",
    "        \"\"\"Build vocabulary for spell checking\"\"\"\n",
    "        print(\"üîß Building vocabulary for spell checking...\")\n",
    "        \n",
    "        # Sample files for vocabulary (avoid processing all for large vaults)\n",
    "        sample_files = list(self.file_contents.items())[:50]\n",
    "        \n",
    "        for file_path, content in sample_files:\n",
    "            words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content.lower())\n",
    "            self.common_words.update(words)\n",
    "        \n",
    "        print(f\"‚úÖ Built vocabulary with {len(self.common_words)} words\")\n",
    "    \n",
    "    def _chunk_document(self, content: str, max_chunk_size: int = 2000) -> List[str]:\n",
    "        \"\"\"Chunk large documents for better semantic search\"\"\"\n",
    "        if len(content) <= max_chunk_size:\n",
    "            return [content]\n",
    "        \n",
    "        # Try to split by paragraphs first\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            # If adding this paragraph would exceed limit, save current chunk\n",
    "            if len(current_chunk) + len(paragraph) > max_chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph\n",
    "            else:\n",
    "                current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def spell_check_keyword(self, keyword):\n",
    "        \"\"\"Try to correct spelling\"\"\"\n",
    "        if not self.common_words:\n",
    "            return keyword\n",
    "            \n",
    "        suggestions = difflib.get_close_matches(\n",
    "            keyword.lower(), \n",
    "            self.common_words, \n",
    "            n=3, \n",
    "            cutoff=0.6\n",
    "        )\n",
    "        \n",
    "        if suggestions and suggestions[0] != keyword.lower():\n",
    "            print(f\"üî§ Did you mean: {', '.join(suggestions[:3])}?\")\n",
    "            return suggestions[0]\n",
    "        \n",
    "        return keyword\n",
    "    \n",
    "    def keyword_search(self, keyword: str) -> List[Dict]:\n",
    "        \"\"\"Traditional keyword search\"\"\"\n",
    "        print(f\"üîç Keyword search for: '{keyword}'\")\n",
    "        \n",
    "        matching_files = []\n",
    "        keyword_lower = keyword.lower()\n",
    "        \n",
    "        for file_path, content in self.file_contents.items():\n",
    "            if keyword_lower in content.lower():\n",
    "                context = self._get_keyword_context(content, keyword)\n",
    "                matching_files.append({\n",
    "                    'file_path': file_path,\n",
    "                    'file_name': Path(file_path).name,\n",
    "                    'folder': str(Path(file_path).parent),\n",
    "                    'context': context,\n",
    "                    'file_size': len(content),\n",
    "                    'search_type': 'keyword',\n",
    "                    'relevance_score': 1.0\n",
    "                })\n",
    "        \n",
    "        return matching_files\n",
    "    \n",
    "    def semantic_search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Semantic similarity search using AI\"\"\"\n",
    "        if not SEMANTIC_SEARCH_AVAILABLE or not self.model:\n",
    "            print(\"‚ùå Semantic search not available. Install sentence-transformers.\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"üß† Semantic search for: '{query}'\")\n",
    "        print(\"   Processing document chunks...\")\n",
    "        \n",
    "        # Create corpus of chunks\n",
    "        all_chunks = []\n",
    "        chunk_info = []\n",
    "        \n",
    "        for file_path, content in self.file_contents.items():\n",
    "            chunks = self._chunk_document(content)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if len(chunk.strip()) > 100:  # Skip very short chunks\n",
    "                    all_chunks.append(chunk)\n",
    "                    chunk_info.append({\n",
    "                        'file_path': file_path,\n",
    "                        'chunk_index': i,\n",
    "                        'chunk_content': chunk\n",
    "                    })\n",
    "        \n",
    "        if not all_chunks:\n",
    "            print(\"‚ùå No content chunks found\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"   Encoding {len(all_chunks)} chunks with AI model...\")\n",
    "        \n",
    "        # Get embeddings\n",
    "        try:\n",
    "            query_embedding = self.model.encode([query])[0]\n",
    "            chunk_embeddings = self.model.encode(all_chunks, show_progress_bar=True)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during encoding: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, chunk_emb in enumerate(chunk_embeddings):\n",
    "            similarity = np.dot(query_embedding, chunk_emb) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb)\n",
    "            )\n",
    "            similarities.append((similarity, chunk_info[i]))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Group by file and get top results\n",
    "        file_results = {}\n",
    "        for similarity, info in similarities:\n",
    "            file_path = info['file_path']\n",
    "            \n",
    "            if file_path not in file_results:\n",
    "                file_results[file_path] = {\n",
    "                    'file_path': file_path,\n",
    "                    'file_name': Path(file_path).name,\n",
    "                    'folder': str(Path(file_path).parent),\n",
    "                    'context': info['chunk_content'][:500] + \"...\",\n",
    "                    'file_size': len(self.file_contents[file_path]),\n",
    "                    'search_type': 'semantic',\n",
    "                    'relevance_score': similarity,\n",
    "                    'best_chunk': info['chunk_content']\n",
    "                }\n",
    "            elif similarity > file_results[file_path]['relevance_score']:\n",
    "                # Update with better matching chunk\n",
    "                file_results[file_path]['relevance_score'] = similarity\n",
    "                file_results[file_path]['context'] = info['chunk_content'][:500] + \"...\"\n",
    "                file_results[file_path]['best_chunk'] = info['chunk_content']\n",
    "        \n",
    "        # Return top results\n",
    "        results = list(file_results.values())\n",
    "        results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        return results[:top_k]\n",
    "    \n",
    "    def smart_search(self, query: str, auto_correct: bool = True) -> List[Dict]:\n",
    "        \"\"\"Automatically choose between keyword and semantic search\"\"\"\n",
    "        # Clean up query\n",
    "        query = query.strip()\n",
    "        \n",
    "        if not query:\n",
    "            return []\n",
    "        \n",
    "        # Decide search type based on query length\n",
    "        word_count = len(query.split())\n",
    "        \n",
    "        if word_count <= 3:\n",
    "            # Short queries: use keyword search\n",
    "            print(f\"üéØ Using KEYWORD search ({word_count} words)\")\n",
    "            results = self.keyword_search(query)\n",
    "            \n",
    "            # Try spell correction if no results\n",
    "            if not results and auto_correct:\n",
    "                corrected = self.spell_check_keyword(query)\n",
    "                if corrected != query.lower():\n",
    "                    print(f\"üîÑ Trying corrected spelling: '{corrected}'\")\n",
    "                    results = self.keyword_search(corrected)\n",
    "        else:\n",
    "            # Long queries: use semantic search\n",
    "            print(f\"üéØ Using SEMANTIC search ({word_count} words)\")\n",
    "            results = self.semantic_search(query)\n",
    "            \n",
    "            # Fallback to keyword search if semantic fails\n",
    "            if not results:\n",
    "                print(\"üîÑ Falling back to keyword search...\")\n",
    "                # Extract key terms from query\n",
    "                key_terms = [word for word in query.split() if len(word) > 3]\n",
    "                if key_terms:\n",
    "                    results = self.keyword_search(\" \".join(key_terms[:2]))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_keyword_context(self, content: str, keyword: str) -> str:\n",
    "        \"\"\"Get context around keyword\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        context_lines = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if keyword.lower() in line.lower():\n",
    "                start = max(0, i-1)\n",
    "                end = min(len(lines), i+2)\n",
    "                context = '\\n'.join(lines[start:end])\n",
    "                context_lines.append(context)\n",
    "                if len(context_lines) >= 2:\n",
    "                    break\n",
    "        \n",
    "        return '\\n...\\n'.join(context_lines) if context_lines else \"No context found\"\n",
    "    \n",
    "    def display_results(self, results: List[Dict]):\n",
    "        \"\"\"Display search results\"\"\"\n",
    "        if not results:\n",
    "            print(\"‚ùå No matching documents found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüìã FOUND {len(results)} RELEVANT DOCUMENTS:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {result['file_name']}\")\n",
    "            print(f\"   üìÇ Folder: {result['folder']}\")\n",
    "            print(f\"   üìè Size: {result['file_size']:,} chars\")\n",
    "            print(f\"   üîç Search: {result['search_type'].upper()}\")\n",
    "            \n",
    "            if result['search_type'] == 'semantic':\n",
    "                print(f\"   üìà Relevance: {result['relevance_score']:.3f}\")\n",
    "            \n",
    "            print(f\"   üìù Preview:\")\n",
    "            context_lines = result['context'].split('\\n')[:3]\n",
    "            for line in context_lines:\n",
    "                print(f\"      {line[:100]}{'...' if len(line) > 100 else ''}\")\n",
    "    \n",
    "    def select_and_view_files(self, results: List[Dict]):\n",
    "        \"\"\"Handle file selection and viewing\"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüéØ SELECT DOCUMENTS (Multiple Selection Supported):\")\n",
    "        print(\"Examples: 1,3,5 | 1-3 | all | quit\")\n",
    "        \n",
    "        while True:\n",
    "            choice = input(f\"\\nYour selection (1-{len(results)}): \").strip()\n",
    "            \n",
    "            if not choice:\n",
    "                print(\"‚ùå Please make a selection\")\n",
    "                continue\n",
    "            \n",
    "            if choice.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Exiting...\")\n",
    "                sys.exit(0)\n",
    "            \n",
    "            if choice.lower() == 'all':\n",
    "                selected = results\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                selected_indices = []\n",
    "                parts = choice.split(',')\n",
    "                \n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if '-' in part:\n",
    "                        start, end = map(int, part.split('-'))\n",
    "                        selected_indices.extend(range(start-1, end))\n",
    "                    else:\n",
    "                        selected_indices.append(int(part) - 1)\n",
    "                \n",
    "                valid_indices = [i for i in selected_indices if 0 <= i < len(results)]\n",
    "                if not valid_indices:\n",
    "                    print(\"‚ùå No valid selections\")\n",
    "                    continue\n",
    "                \n",
    "                selected = [results[i] for i in valid_indices]\n",
    "                break\n",
    "                \n",
    "            except ValueError:\n",
    "                print(\"‚ùå Invalid format\")\n",
    "                continue\n",
    "        \n",
    "        # View selected files\n",
    "        print(f\"\\nüìö VIEWING {len(selected)} SELECTED DOCUMENTS:\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        for i, result in enumerate(selected, 1):\n",
    "            print(f\"\\nüìñ DOCUMENT {i}/{len(selected)}: {result['file_name']}\")\n",
    "            print(f\"üìÇ Path: {result['file_path']}\")\n",
    "            print(f\"üîç Found via: {result['search_type'].upper()} search\")\n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            try:\n",
    "                with open(result['file_path'], 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    print(content)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error reading file: {e}\")\n",
    "            \n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            if i < len(selected):\n",
    "                user_input = input(f\"\\nPress Enter to view next document or 'quit' to exit: \").strip().lower()\n",
    "                if user_input in ['quit', 'exit', 'q']:\n",
    "                    print(\"üëã Exiting...\")\n",
    "                    sys.exit(0)\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Main interactive interface\"\"\"\n",
    "        while True:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ü§ñ SMART DOCUMENT FINDER\")\n",
    "            print(\"‚ú® Features: Auto keyword/semantic search ‚Ä¢ Multi-select ‚Ä¢ Spell check\")\n",
    "            print(f\"üìÅ Vault: {len(self.file_contents)} documents loaded\")\n",
    "            if SEMANTIC_SEARCH_AVAILABLE:\n",
    "                print(\"üß† AI semantic search: ENABLED\")\n",
    "            else:\n",
    "                print(\"üß† AI semantic search: DISABLED (install sentence-transformers)\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            query = input(\"\\nüí≠ Enter your search query: \").strip()\n",
    "            \n",
    "            if not query:\n",
    "                print(\"‚ùå Please enter a query\")\n",
    "                continue\n",
    "            \n",
    "            if query.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Perform smart search\n",
    "            results = self.smart_search(query)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"‚ùå No documents found. Try a different query.\")\n",
    "                continue\n",
    "            \n",
    "            # Display and handle selection\n",
    "            self.display_results(results)\n",
    "            self.select_and_view_files(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        finder = SmartDocumentFinder()\n",
    "        finder.interactive_search()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Program interrupted. Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c803e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
