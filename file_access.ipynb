{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eda2813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\7. Projects From Sem 3\\RAG\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc0e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "class SimpleKeywordFinder:\n",
    "    def __init__(self, vault_path):\n",
    "        self.vault_path = Path(vault_path)\n",
    "        print(f\"üìÅ Vault path: {vault_path}\")\n",
    "        \n",
    "    def search_keyword(self, keyword):\n",
    "        \"\"\"Simple keyword search through all .md files\"\"\"\n",
    "        print(f\"\\nüîç Searching for keyword: '{keyword}'\")\n",
    "        \n",
    "        # Get all .md files\n",
    "        md_files = list(self.vault_path.rglob(\"*.md\"))\n",
    "        print(f\"üìö Scanning {len(md_files)} markdown files...\")\n",
    "        \n",
    "        if not md_files:\n",
    "            print(\"‚ùå No .md files found in vault!\")\n",
    "            return []\n",
    "        \n",
    "        matching_files = []\n",
    "        processed = 0\n",
    "        \n",
    "        for file_path in md_files:\n",
    "            processed += 1\n",
    "            if processed % 100 == 0:  # Progress for large vaults\n",
    "                print(f\"   Processed {processed}/{len(md_files)} files...\")\n",
    "                \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                # Simple case-insensitive search\n",
    "                if keyword.lower() in content.lower():\n",
    "                    # Find context around the keyword\n",
    "                    context = self.get_keyword_context(content, keyword)\n",
    "                    \n",
    "                    matching_files.append({\n",
    "                        'file_path': str(file_path),\n",
    "                        'file_name': file_path.name,\n",
    "                        'folder': str(file_path.parent),\n",
    "                        'context': context,\n",
    "                        'file_size': len(content)\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error reading {file_path}: {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ Search complete! Found {len(matching_files)} files containing '{keyword}'\")\n",
    "        return matching_files\n",
    "    \n",
    "    def get_keyword_context(self, content, keyword):\n",
    "        \"\"\"Get text around the keyword for preview\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        context_lines = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if keyword.lower() in line.lower():\n",
    "                # Get surrounding lines for context\n",
    "                start = max(0, i-1)\n",
    "                end = min(len(lines), i+2)\n",
    "                context = '\\n'.join(lines[start:end])\n",
    "                context_lines.append(context)\n",
    "                \n",
    "                if len(context_lines) >= 2:  # Limit context snippets\n",
    "                    break\n",
    "        \n",
    "        return '\\n...\\n'.join(context_lines) if context_lines else \"No context found\"\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive keyword search\"\"\"\n",
    "        while True:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üîç KEYWORD FILE FINDER\")\n",
    "            print(\"Find .md files containing your keyword\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            keyword = input(\"\\nüí≠ Enter keyword to search: \").strip()\n",
    "            \n",
    "            if not keyword:\n",
    "                print(\"‚ùå Please enter a keyword\")\n",
    "                continue\n",
    "                \n",
    "            if keyword.lower() in ['quit', 'exit', 'stop']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Search for keyword\n",
    "            matching_files = self.search_keyword(keyword)\n",
    "            \n",
    "            if not matching_files:\n",
    "                print(f\"‚ùå No files found containing '{keyword}'\")\n",
    "                continue\n",
    "            \n",
    "            # Show results\n",
    "            print(f\"\\nüìã FOUND {len(matching_files)} FILES:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for i, file_info in enumerate(matching_files, 1):\n",
    "                print(f\"\\n{i}. {file_info['file_name']}\")\n",
    "                print(f\"   üìÇ Folder: {file_info['folder']}\")\n",
    "                print(f\"   üìè Size: {file_info['file_size']} chars\")\n",
    "                print(f\"   üìù Context:\")\n",
    "                print(f\"      {file_info['context'][:200]}...\")\n",
    "                \n",
    "            # Let user choose\n",
    "            print(f\"\\nüéØ SELECT A FILE:\")\n",
    "            choice = input(f\"Enter number (1-{len(matching_files)}) or 'new' for new search: \").strip()\n",
    "            \n",
    "            if choice.lower() == 'new':\n",
    "                continue\n",
    "                \n",
    "            if choice.isdigit() and 1 <= int(choice) <= len(matching_files):\n",
    "                selected_file = matching_files[int(choice) - 1]\n",
    "                self.show_full_file(selected_file)\n",
    "            else:\n",
    "                print(\"‚ùå Invalid choice\")\n",
    "    \n",
    "    def show_full_file(self, file_info):\n",
    "        \"\"\"Show full content of selected file\"\"\"\n",
    "        print(f\"\\nüìñ FULL CONTENT: {file_info['file_name']}\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        try:\n",
    "            with open(file_info['file_path'], 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(content)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading file: {e}\")\n",
    "            \n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Ask what to do next\n",
    "        action = input(\"\\nWhat next? (s)earch again, (q)uit: \").strip().lower()\n",
    "        if action == 'q':\n",
    "            return False  # Signal to quit\n",
    "        return True  # Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9302a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Simple Keyword File Finder\n",
      "Perfect for large Joplin vaults!\n",
      "üìÅ Vault path: D:\\LOST.DIR\\Obsidian Vault\n",
      "\n",
      "============================================================\n",
      "üîç KEYWORD FILE FINDER\n",
      "Find .md files containing your keyword\n",
      "============================================================\n",
      "\n",
      "üîç Searching for keyword: 'query tranformation'\n",
      "üìö Scanning 227 markdown files...\n",
      "   Processed 100/227 files...\n",
      "   Processed 200/227 files...\n",
      "‚úÖ Search complete! Found 0 files containing 'query tranformation'\n",
      "‚ùå No files found containing 'query tranformation'\n",
      "\n",
      "============================================================\n",
      "üîç KEYWORD FILE FINDER\n",
      "Find .md files containing your keyword\n",
      "============================================================\n",
      "\n",
      "üîç Searching for keyword: 'query transformation'\n",
      "üìö Scanning 227 markdown files...\n",
      "   Processed 100/227 files...\n",
      "   Processed 200/227 files...\n",
      "‚úÖ Search complete! Found 3 files containing 'query transformation'\n",
      "\n",
      "üìã FOUND 3 FILES:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Rag(Query Transformation ).md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\11. RAG\n",
      "   üìè Size: 7122 chars\n",
      "   üìù Context:\n",
      "      ---\n",
      "title: Rag(Query Transformation )\n",
      "updated: 2025-05-07 17:57:42Z...\n",
      "\n",
      "2. Recursive answering and follow up questioning.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\15. Holiday Studying GenAI\\Articles on RAG\n",
      "   üìè Size: 13383 chars\n",
      "   üìù Context:\n",
      "      \n",
      "The basic idea is that when we perform query transformation , and generate sub query from the users query then and answer that question so during it answer we make a question answering pair where we ...\n",
      "\n",
      "3. 1. Text based Daily note.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\5. Project Ideas\n",
      "   üìè Size: 895 chars\n",
      "   üìù Context:\n",
      "      \n",
      "So it would be an ai powered application where ill solve my problem which i have to deal with. Personally i want to make an app where im just able to dumb down text of what i did on that day and the ...\n",
      "\n",
      "üéØ SELECT A FILE:\n",
      "‚ùå Invalid choice\n",
      "\n",
      "============================================================\n",
      "üîç KEYWORD FILE FINDER\n",
      "Find .md files containing your keyword\n",
      "============================================================\n",
      "‚ùå Please enter a keyword\n",
      "\n",
      "============================================================\n",
      "üîç KEYWORD FILE FINDER\n",
      "Find .md files containing your keyword\n",
      "============================================================\n",
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† Simple Keyword File Finder\")\n",
    "    print(\"Perfect for large Joplin vaults!\")\n",
    "    \n",
    "    vault_path = input(\"\\nEnter path to your vault: \").strip()\n",
    "    \n",
    "    if not vault_path:\n",
    "        print(\"‚ùå Please provide a valid path\")\n",
    "        exit()\n",
    "    \n",
    "    if not Path(vault_path).exists():\n",
    "        print(f\"‚ùå Path {vault_path} doesn't exist\")\n",
    "        exit()\n",
    "    \n",
    "    finder = SimpleKeywordFinder(vault_path)\n",
    "    finder.interactive_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8078dee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Enhanced Multi-Select Keyword Finder\n",
      "Perfect for large Joplin vaults with spell checking!\n",
      "üìÅ Vault path: D:\\LOST.DIR\\Obsidian Vault\n",
      "üîß Building vocabulary for spell checking...\n",
      "‚úÖ Built vocabulary with 2984 words\n",
      "\n",
      "================================================================================\n",
      "üîç ENHANCED KEYWORD FILE FINDER\n",
      "‚ú® Features: Multi-select ‚Ä¢ View multiple files ‚Ä¢ Spell checking\n",
      "================================================================================\n",
      "\n",
      "üîç Searching for keyword: 'agets'\n",
      "   Processed 100/227 files...\n",
      "   Processed 200/227 files...\n",
      "üî§ Did you mean: agents, gets, agent?\n",
      "üîÑ Trying corrected spelling: 'agents'\n",
      "\n",
      "üîç Searching for keyword: 'agents'\n",
      "   Processed 100/227 files...\n",
      "   Processed 200/227 files...\n",
      "‚úÖ Found 20 files containing 'agents'\n",
      "\n",
      "üìã FOUND 20 FILES:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. 1. E-commerce website shittt.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\1. Projects\\1. Big Data Project Ideas\n",
      "   üìè Size: 5,635 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      - Customers expect **instant replies** from chatbots or human agents.\n",
      "          \n",
      "\n",
      "2. 0. Routine.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\10. Important shittt\n",
      "   üìè Size: 1,494 chars\n",
      "   üìù Context Preview:\n",
      "      \t- Always do some or the other courses:\n",
      "      \t\t- AI agents\n",
      "      \t\t- MCP\n",
      "\n",
      "3. RAG.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\11. RAG\n",
      "   üìè Size: 13,690 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      `\"What is task decomposition for LLM agents?\" ‚Üí (LLM generates something like) \"what is task decompo...\n",
      "      \n",
      "\n",
      "4. Things left to do in RAG.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\11. RAG\n",
      "   üìè Size: 3,629 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      **LangGraph** is an **open-source framework** built on top of LangChain that enables you to build **...\n",
      "      \n",
      "\n",
      "5. Technology and the intution.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\13. Agentic AI\n",
      "   üìè Size: 3,042 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      `from langchain.agents import initialize_agent, Tool from langchain.tools import WikipediaAPIWrapper...\n",
      "      \n",
      "\n",
      "6. Types.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\13. Agentic AI\n",
      "   üìè Size: 2,906 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      - Multiple agents working **together** in the same environment.\n",
      "          \n",
      "\n",
      "7. Google Books to Read.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\15. Holiday Studying GenAI\n",
      "   üìè Size: 429 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      ![[22365_19_Agents_v8.pdf]]\n",
      "      \n",
      "\n",
      "8. TO READ CHROME TABS.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\15. Holiday Studying GenAI\n",
      "   üìè Size: 718 chars\n",
      "   üìù Context Preview:\n",
      "      https://www.sciencedirect.com/science/article/abs/pii/S1566253520303651\n",
      "      https://web-and-mobile-development.medium.com/how-to-create-an-ai-chatbot-a-comprehensive-guide-to-b...\n",
      "      https://www.anthropic.com/engineering/building-effective-agents\n",
      "\n",
      "9. Agents.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\n",
      "   üìè Size: 3,873 chars\n",
      "   üìù Context Preview:\n",
      "      ---\n",
      "      title: Agents\n",
      "      updated: 2025-06-06 04:19:50Z\n",
      "\n",
      "10. TO--DO.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\n",
      "   üìè Size: 682 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      https://huggingface.co/docs/smolagents/main/en/examples/multiagents\n",
      "\n",
      "11. Intro.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\Langraphs\n",
      "   üìè Size: 2,104 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      ### [](https://huggingface.co/learn/agents-course/unit2/langgraph/when_to_use_langgraph#control-vs-f...\n",
      "      \n",
      "\n",
      "12. Intro.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\MCP\n",
      "   üìè Size: 1,865 chars\n",
      "   üìù Context Preview:\n",
      "      ¬†**M√óN Integration Problem**\n",
      "      - **Host**: The user-facing AI application that end-users interact with directly. Examples include A...\n",
      "          \n",
      "\n",
      "13. QueryEngine.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\Summary LlamaIndex\n",
      "   üìè Size: 3,075 chars\n",
      "   üìù Context Preview:\n",
      "      1. `FunctionTool`: Convert any Python function into a tool that an agent can use. It automatically f...\n",
      "      2. `QueryEngineTool`: A tool that lets agents use query engines. Since agents are built on query eng...\n",
      "      3. `Toolspecs`: Sets of tools created by the community, which often include tools for specific servi...\n",
      "\n",
      "14. Smolagent.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\Summary SmolAgent\n",
      "   üìè Size: 1,707 chars\n",
      "   üìù Context Preview:\n",
      "      > \n",
      "         https://huggingface.co/docs/smolagents/index -- the documentation\n",
      "      ---\n",
      "\n",
      "15. VisionTransformers.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\Summary SmolAgent\n",
      "   üìè Size: 847 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      - [We just gave sight to smolagents](https://huggingface.co/blog/smolagents-can-see)¬†- Blog describi...\n",
      "      - [Web Browser Automation with Agents ü§ñüåê](https://huggingface.co/docs/smolagents/examples/web_browse...\n",
      "\n",
      "16. RAG+.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\19. Summary Of Research Paper\n",
      "   üìè Size: 2,327 chars\n",
      "   üìù Context Preview:\n",
      "      7.  Agentic RAG (Singh et al., 2025)\n",
      "      8. RAG-Gym (Xiong et al., 2025) decompose tasks into subtasks han- dled by specialized agents or wor...\n",
      "      9. OPEN- RAG (Islam et al., 2024) introduces agent-based de- composition and context selection\n",
      "\n",
      "17. When was in Artemis.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\2. Chrome\n",
      "   üìè Size: 1,018 chars\n",
      "   üìù Context Preview:\n",
      "       https://transformer-circuits.pub/2025/attribution-graphs/methods.html\n",
      "       https://huggingface.co/blog/tiny-agents\n",
      "       https://www.continue.dev/about-us\n",
      "\n",
      "18. 2025-05-28.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\20. Stuff-I-Don_t-Need\n",
      "   üìè Size: 2,482 chars\n",
      "   üìù Context Preview:\n",
      "      \tSystem Overview\n",
      "      \tYou will build 3 agents, orchestrated via a central Classifier Agent:\n",
      "      \t\n",
      "\n",
      "19. 1. Intro.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\4. MCP\n",
      "   üìè Size: 1,756 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      Model Context Protocol (MCP) is an open standard developed by Anthropic, the company behind Claude. ...\n",
      "      \n",
      "\n",
      "20. chatbot with message summarizer.md\n",
      "   üìÇ Folder: D:\\LOST.DIR\\Obsidian Vault\\6. LangGraphs\\module-2\n",
      "   üìè Size: 690 chars\n",
      "   üìù Context Preview:\n",
      "      \n",
      "      Trim and filter message which the agents sends to the llm its that shit , so how do we revoke the ac...\n",
      "      this video shows the integration of all we have learned so far\n",
      "\n",
      "üéØ SELECT FILES (Multiple Selection Supported):\n",
      "Examples:\n",
      "  ‚Ä¢ Single: 1\n",
      "  ‚Ä¢ Multiple: 1,3,5\n",
      "  ‚Ä¢ Range: 1-3\n",
      "  ‚Ä¢ All: all\n",
      "\n",
      "üéØ What would you like to do with 20 selected files?\n",
      "1. View file contents\n",
      "2. Search for different keyword\n",
      "3. Quit\n",
      "\n",
      "üìö VIEWING 20 SELECTED FILES:\n",
      "====================================================================================================\n",
      "\n",
      "üìñ FILE 1/20: 1. E-commerce website shittt.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\1. Projects\\1. Big Data Project Ideas\\1. E-commerce website shittt.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: 1. E-commerce website shittt\n",
      "updated: 2025-04-01 05:27:48Z\n",
      "created: 2025-03-31 10:11:45Z\n",
      "---\n",
      "\n",
      "Kafka can **power different parts of an e-commerce platform**, including:\n",
      "\n",
      "### **1Ô∏è‚É£ Real-Time Order Processing & Inventory Management**\n",
      "\n",
      "üìå **Problem:**\n",
      "\n",
      "- Multiple users **place orders simultaneously**.\n",
      "    \n",
      "- The inventory system must **instantly update stock levels** across multiple warehouses.\n",
      "    \n",
      "- Without Kafka, **database locking or slow queries can cause issues**.\n",
      "    \n",
      "\n",
      "üìå **Kafka Solution:**\n",
      "\n",
      "- When a user **places an order**, an **Order Service (producer)** sends a message to Kafka.\n",
      "    \n",
      "- The **Inventory Service (consumer)** listens for new orders and updates stock levels.\n",
      "    \n",
      "- If an item **goes out of stock**, Kafka triggers an alert.\n",
      "    \n",
      "\n",
      "üìå **Flow:**\n",
      "\n",
      "scss\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`User Places Order ‚Üí Kafka (orders_topic) ‚Üí Order Processing Service ‚Üí Kafka (inventory_topic) ‚Üí Inventory Service Updates Stock`\n",
      "\n",
      "üîπ **Why Kafka?** ‚Üí Ensures no orders are lost, handles high traffic efficiently.\n",
      "\n",
      "---\n",
      "\n",
      "### **2Ô∏è‚É£ Real-Time Product Recommendations (Personalization Engine)**\n",
      "\n",
      "üìå **Problem:**\n",
      "\n",
      "- Users browse different products, but recommendations should be **updated dynamically**.\n",
      "    \n",
      "- Traditional batch-based recommendations **feel outdated** compared to real-time systems.\n",
      "    \n",
      "\n",
      "üìå **Kafka Solution:**\n",
      "\n",
      "- User activity (**clicks, views, add-to-cart, purchases**) is **streamed** into Kafka.\n",
      "    \n",
      "- A **Recommendation Engine (consumer)** processes this in real time.\n",
      "    \n",
      "- The **next time the user visits**, they see **personalized suggestions**.\n",
      "    \n",
      "\n",
      "üìå **Flow:**\n",
      "\n",
      "scss\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`User Clicks on Product ‚Üí Kafka (user_activity_topic) ‚Üí Recommendation Engine ‚Üí Kafka (recommendations_topic) ‚Üí Frontend Fetches Updated Recommendations`\n",
      "\n",
      "üîπ **Why Kafka?** ‚Üí Streams user behavior in **real time**, improving recommendation accuracy.\n",
      "\n",
      "---\n",
      "\n",
      "### **3Ô∏è‚É£ Fraud Detection & Payment Security**\n",
      "\n",
      "üìå **Problem:**\n",
      "\n",
      "- Fraudulent transactions (e.g., multiple high-value orders from different locations).\n",
      "    \n",
      "- **Traditional fraud detection** relies on batch processing, leading to delays.\n",
      "    \n",
      "\n",
      "üìå **Kafka Solution:**\n",
      "\n",
      "- Every **payment request** is sent to Kafka.\n",
      "    \n",
      "- A **Fraud Detection System (consumer)** monitors transactions **in real time**.\n",
      "    \n",
      "- If a transaction looks **suspicious**, Kafka triggers an **alert** or **blocks it**.\n",
      "    \n",
      "\n",
      "üìå **Flow:**\n",
      "\n",
      "pgsql\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`User Makes Payment ‚Üí Kafka (payment_topic) ‚Üí Fraud Detection System ‚Üí Kafka (alerts_topic) ‚Üí Alerts Admin or Blocks Transaction`\n",
      "\n",
      "üîπ **Why Kafka?** ‚Üí Instant fraud detection, preventing chargebacks and losses.\n",
      "\n",
      "---\n",
      "\n",
      "### **4Ô∏è‚É£ Real-Time Delivery Tracking (For Quick Commerce like Blinkit)**\n",
      "\n",
      "üìå **Problem:**\n",
      "\n",
      "- Delivery updates should be **live**, not refreshed every few minutes.\n",
      "    \n",
      "- Customers expect **instant tracking** (like Blinkit‚Äôs 10-minute delivery model).\n",
      "    \n",
      "\n",
      "üìå **Kafka Solution:**\n",
      "\n",
      "- The **Delivery System (producer)** sends updates to Kafka whenever the order moves (e.g., **packed ‚Üí out for delivery ‚Üí arriving**).\n",
      "    \n",
      "- The **Frontend (consumer)** listens to Kafka and **updates the tracking page** dynamically.\n",
      "    \n",
      "\n",
      "üìå **Flow:**\n",
      "\n",
      "sql\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`Delivery Status Updates ‚Üí Kafka (delivery_topic) ‚Üí Customer Tracking Dashboard Updates in Real Time`\n",
      "\n",
      "üîπ **Why Kafka?** ‚Üí Ensures **live updates** without refreshing the app.\n",
      "\n",
      "---\n",
      "\n",
      "### **5Ô∏è‚É£ Handling Flash Sales & High Traffic (Scalability)**\n",
      "\n",
      "üìå **Problem:**\n",
      "\n",
      "- During sales (Big Billion Days, Amazon Prime Day), **millions of users** hit the platform at once.\n",
      "    \n",
      "- If traffic is **directly sent to databases**, it can cause **crashes or slowdowns**.\n",
      "    \n",
      "\n",
      "üìå **Kafka Solution:**\n",
      "\n",
      "- **Instead of directly writing orders to the database, store them in Kafka first.**\n",
      "    \n",
      "- A background **Order Processing System** picks up orders from Kafka and processes them gradually.\n",
      "    \n",
      "- **Even if the database slows down, orders are not lost** because they are queued in Kafka.\n",
      "    \n",
      "\n",
      "üìå **Flow:**\n",
      "\n",
      "pgsql\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`Users Place Orders ‚Üí Kafka (order_queue) ‚Üí Order Processing System ‚Üí Database`\n",
      "\n",
      "üîπ **Why Kafka?** ‚Üí **Acts as a buffer**, preventing system crashes during high traffic.\n",
      "\n",
      "---\n",
      "\n",
      "### **6Ô∏è‚É£ Customer Support Chatbots & Real-Time Messaging**\n",
      "\n",
      "üìå **Problem:**\n",
      "\n",
      "- Customers expect **instant replies** from chatbots or human agents.\n",
      "    \n",
      "- Traditional request-response APIs can **become slow under heavy load**.\n",
      "    \n",
      "\n",
      "üìå **Kafka Solution:**\n",
      "\n",
      "- Customer messages are **sent to Kafka**, so chatbots/human agents **process them asynchronously**.\n",
      "    \n",
      "- Customers receive **quick responses, even if an agent isn‚Äôt available immediately**.\n",
      "    \n",
      "\n",
      "üìå **Flow:**\n",
      "\n",
      "sql\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`User Message ‚Üí Kafka (chat_messages_topic) ‚Üí Chatbot/Support Team ‚Üí Kafka (response_topic) ‚Üí User Gets Reply`\n",
      "\n",
      "üîπ **Why Kafka?** ‚Üí Handles **high chat volume** without delays.\n",
      "\n",
      "---\n",
      "\n",
      "## **üîπ TL;DR: Why Kafka for E-Commerce?**\n",
      "\n",
      "‚úî **Handles Real-Time Orders** ‚Üí Ensures stock levels update instantly.  \n",
      "‚úî **Powers Personalization** ‚Üí Streams user interactions for better recommendations.  \n",
      "‚úî **Prevents Fraud** ‚Üí Analyzes transactions as they happen.  \n",
      "‚úî **Improves Delivery Tracking** ‚Üí Live updates on order status.  \n",
      "‚úî **Handles High Traffic** ‚Üí Prevents crashes during flash sales.  \n",
      "‚úî **Supports Live Chat** ‚Üí Makes chatbots and customer support instant.\n",
      "\n",
      "---\n",
      "\n",
      "### **üîπ Next Step: What Should We Build?**\n",
      "\n",
      "Now that we know **how Kafka fits into e-commerce**, we need to decide **what part to implement**:\n",
      "\n",
      "1Ô∏è‚É£ **Real-time Order Processing & Inventory Management**  \n",
      "2Ô∏è‚É£ **Live Product Recommendations**  \n",
      "3Ô∏è‚É£ **Fraud Detection System**  \n",
      "4Ô∏è‚É£ **Delivery Tracking for Quick Commerce**  \n",
      "5Ô∏è‚É£ **Handling High Traffic During Sales**\n",
      " \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 2/20: 0. Routine.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\10. Important shittt\\0. Routine.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: 0. Routine\n",
      "updated: 2025-06-05 20:12:03Z\n",
      "created: 2025-06-05 19:58:32Z\n",
      "---\n",
      "\n",
      "- Morning Routine \n",
      "\t- Wake up and have alot of water\n",
      "\t- Get sunlight into your eyes , the rising sun\n",
      "\t- Do move your body and increase your heartbeat \n",
      "\t- Do kegel\n",
      "\t- First 90 Min of the morning are the one where you are most active so why not utilize that if you are able to do that otherwise will just well squeeze in time for like 30 min or so.\n",
      "\t- Take a bath each day and dress up in the most confident way , wear your confidence\n",
      "\n",
      "- Education\n",
      "\t- Stay updated with what is going on in the world ie in genai not as such stuff, so twitter , linkedin and medium articles are the only sources here. \n",
      "\t\t- USE: \n",
      "\t\t\t- Twitter atleast once\n",
      "\t\t\t- Linkedin atleast once \n",
      "\t\t\t- Read one article daily\n",
      "\t- Always do some or the other courses:\n",
      "\t\t- AI agents\n",
      "\t\t- MCP\n",
      "\t\t- Model Thinking\n",
      "\t\t- LLM\n",
      "\t\t- Human Centric stuff on io. see the obsidian repo\n",
      "- Extra Stuff to take care of\n",
      "\t- Read Books\n",
      "\t- Listen to Podcasts , what will you do while you eat? \n",
      "\t- Revise all the stuff that you learned and summarize all the notes that you have made here on obsidian as well\n",
      "\n",
      "\n",
      "Study for your Artemis Internship\n",
      "Study for your DNA internship \n",
      "Study for College as well\n",
      "Study for self and intellectual sufficiency\n",
      "\n",
      "\n",
      "Before Going -- today is 6th June\n",
      "\tRevise the entire ML from the campusX notes\n",
      "\tWatch a video or something on work place etiquette\n",
      "\tFinish that thinkschool course on communication \n",
      "\tKnow your models as well . esp cnn here.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 3/20: RAG.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\11. RAG\\RAG.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: RAG\n",
      "updated: 2025-05-09 07:03:10Z\n",
      "created: 2025-05-07 10:36:05Z\n",
      "---\n",
      "\n",
      "RAG todo:\n",
      "   - Fusion\n",
      "   - Decomposition \n",
      "   - Multi-Query\n",
      "   - Step Back\n",
      "\n",
      "so rag fusion is basically just , generating multiple queries of the user-input-query and then acc to each query get the relevant chunks and then calculate this rrf and rank these doc acc to score and take top 3 or 4 or n and paas it to the llm to generate actual output\n",
      "\n",
      "Difference btw rag fusion and decompostion:\n",
      "\n",
      "### You're Right That:\n",
      "\n",
      "- **RAG Fusion = Multi-query RAG + Reciprocal Rank Fusion (RRF)** ‚Äî Yes.\n",
      "    \n",
      "- **Query Expansion via LLM** ‚Üí generate **multiple semantically related queries** from the original input ‚Äî Yes.\n",
      "    \n",
      "- Each of those is used to **retrieve chunks separately**, and results are **fused via RRF** ‚Äî Yes.\n",
      "    \n",
      "- The **reranked top-N documents** are then passed as context to the LLM for answering ‚Äî Yes.\n",
      "    \n",
      "- This **increases diversity** and reduces reliance on only the top results from a single query ‚Äî Exactly right.\n",
      "\n",
      "\n",
      "## **RAG Fusion = Multi-query RAG + Reciprocal Rank Fusion (RRF)**\n",
      "\n",
      "### üîπ 1. **User Input Query**\n",
      "\n",
      "> `\"impact of climate change\"`\n",
      "\n",
      "---\n",
      "\n",
      "### üîπ 2. **Query Expansion (Query Generation)**\n",
      "\n",
      "Generate **multiple variations** of the query using an LLM:\n",
      "\n",
      "- `\"economic effects of climate change\"`\n",
      "    \n",
      "- `\"how climate change affects biodiversity\"`\n",
      "    \n",
      "- `\"social consequences of global warming\"`  \n",
      "    (etc.)\n",
      "    \n",
      "\n",
      "---\n",
      "\n",
      "### üîπ 3. **Retrieve Chunks for Each Query**\n",
      "\n",
      "Send each query to a retriever (vector store) ‚Üí Get top-k documents/chunks per query.\n",
      "\n",
      "Now you have multiple **ranked lists** of documents.\n",
      "\n",
      "---\n",
      "\n",
      "### üîπ 4. **Reciprocal Rank Fusion (RRF)**\n",
      "\n",
      "Combine those multiple ranked lists using **RRF scoring**:\n",
      "\n",
      "- Gives higher scores to docs that appear in **many lists** and at **higher ranks**.\n",
      "    \n",
      "\n",
      "‚Üí Final output: **One reranked list** of most relevant documents.\n",
      "\n",
      "---\n",
      "\n",
      "### üîπ 5. **Select Top-N Docs**\n",
      "\n",
      "Take the **top N** reranked documents (e.g. top 3 or 5).\n",
      "\n",
      "---\n",
      "\n",
      "### üîπ 6. **Feed to LLM**\n",
      "\n",
      "Use those documents as context in a final prompt like:\n",
      "\n",
      "diff\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`Context: - doc1 - doc2 - doc3  Question: What is the impact of climate change?`\n",
      "\n",
      "‚û°Ô∏è The LLM now answers the original question with the **most contextually relevant**, diverse evidence.\n",
      "\n",
      "---\n",
      "\n",
      "‚úÖ So yes, **RAG Fusion is just a smarter RAG pipeline** that:\n",
      "\n",
      "- Uses multiple interpretations of the query\n",
      "    \n",
      "- Fuses results to avoid narrow or one-sided answers\n",
      "    \n",
      "- Improves both **recall** and **precision** in retrieval\n",
      "\n",
      "\n",
      "i dont understand Reciprocal Rank Fusion here , i mean for each query we are getting seperate documents right? what happens next?\n",
      "\n",
      "###### ChatGPT said:\n",
      "\n",
      "Great question ‚Äî and you're exactly at the core of understanding **Reciprocal Rank Fusion (RRF)**. Let‚Äôs break it down **clearly** with a hands-on, intuitive example.\n",
      "\n",
      "---\n",
      "\n",
      "## üß† Problem Setup:\n",
      "\n",
      "You have **one user query**:\n",
      "\n",
      "> `\"impact of climate change\"`\n",
      "\n",
      "LLM generates **3 alternate search queries**:\n",
      "\n",
      "1. `\"economic effects of climate change\"`\n",
      "    \n",
      "2. `\"how climate change affects biodiversity\"`\n",
      "    \n",
      "3. `\"social consequences of global warming\"`\n",
      "    \n",
      "\n",
      "Each of these queries is sent to a retriever.  \n",
      "Let‚Äôs say **top-3 documents** are returned for each query.\n",
      "\n",
      "---\n",
      "\n",
      "### üì• Retrieval Results (ranked lists)\n",
      "\n",
      "python\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`# For query 1: [\"doc1\", \"doc5\", \"doc3\"]  # For query 2: [\"doc6\", \"doc1\", \"doc4\"]  # For query 3: [\"doc3\", \"doc10\", \"doc1\"]`\n",
      "\n",
      "> You now have 3 lists of results. Each list has **ranked documents**, where position = relevance for that query.\n",
      "\n",
      "---\n",
      "\n",
      "## üéØ Goal of RRF:\n",
      "\n",
      "Combine these **3 lists** into **one final ranked list**, by giving **higher score to documents that appear in multiple lists and rank high**.\n",
      "\n",
      "---\n",
      "\n",
      "## üßÆ How RRF Works:\n",
      "\n",
      "Formula:\n",
      "\n",
      "ini\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`Score = sum(1 / (rank + k)) across all lists`\n",
      "\n",
      "Let‚Äôs assume `k = 60` (a constant to smooth the scores and prevent division by zero).\n",
      "\n",
      "### ‚ûï Step-by-step scoring:\n",
      "\n",
      "#### üìù doc1:\n",
      "\n",
      "- Appears in list 1 at rank 0 ‚Üí 1/(0+60) = 0.0167\n",
      "    \n",
      "- Appears in list 2 at rank 1 ‚Üí 1/(1+60) = 0.0164\n",
      "    \n",
      "- Appears in list 3 at rank 2 ‚Üí 1/(2+60) = 0.0161  \n",
      "    ‚Üí Total: **0.0167 + 0.0164 + 0.0161 = 0.0492**\n",
      "    \n",
      "\n",
      "#### üìù doc3:\n",
      "\n",
      "- Appears in list 1 at rank 2 ‚Üí 1/(2+60) = 0.0161\n",
      "    \n",
      "- Appears in list 3 at rank 0 ‚Üí 1/(0+60) = 0.0167  \n",
      "    ‚Üí Total: **0.0161 + 0.0167 = 0.0328**\n",
      "    \n",
      "\n",
      "#### üìù doc5:\n",
      "\n",
      "- Only in list 1 at rank 1 ‚Üí 1/(1+60) = 0.0164\n",
      "    \n",
      "\n",
      "#### üìù doc6:\n",
      "\n",
      "- Only in list 2 at rank 0 ‚Üí 1/(0+60) = 0.0167\n",
      "    \n",
      "\n",
      "#### üìù doc4:\n",
      "\n",
      "- Only in list 2 at rank 2 ‚Üí 1/(2+60) = 0.0161\n",
      "    \n",
      "\n",
      "#### üìù doc10:\n",
      "\n",
      "- Only in list 3 at rank 1 ‚Üí 1/(1+60) = 0.0164\n",
      "    \n",
      "\n",
      "---\n",
      "\n",
      "## üèÅ Final Fused Scores\n",
      "\n",
      "|Document|Score|\n",
      "|---|---|\n",
      "|`doc1`|0.0492 ‚úÖ highest|\n",
      "|`doc3`|0.0328|\n",
      "|`doc5`|0.0164|\n",
      "|`doc6`|0.0167|\n",
      "|`doc4`|0.0161|\n",
      "|`doc10`|0.0164|\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ Final Reranked List:\n",
      "\n",
      "python\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`[     doc1,     doc3,     doc6,     doc5,     doc10,     doc4 ]`\n",
      "\n",
      "---\n",
      "\n",
      "### üí° Summary:\n",
      "\n",
      "- RRF **does not care** which query a doc came from.\n",
      "    \n",
      "- It only cares **how many times** a document appears across ranked lists, and **how high** it was ranked.\n",
      "    \n",
      "- This encourages diversity and rewards documents **relevant to multiple interpretations** of the user‚Äôs query.\n",
      "    \n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to implement this in code using mock data?\n",
      "\n",
      "4o\n",
      "\n",
      "##### You said:\n",
      "\n",
      "what happens next? what happens after we get this reranked list here\n",
      "\n",
      "###### ChatGPT said:\n",
      "\n",
      "Great ‚Äî now that we have this **reranked list of documents** using Reciprocal Rank Fusion (RRF), let's talk about **what happens next**.\n",
      "\n",
      "---\n",
      "\n",
      "## üîÑ After RRF: Two Typical Options\n",
      "\n",
      "### ‚úÖ Option 1: Use this reranked list in a **final RAG generation step**\n",
      "\n",
      "You take the top-k documents (say top 3) from the reranked list and **feed them into an LLM** with a prompt like:\n",
      "\n",
      "css\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`You are a helpful assistant. Use the following documents to answer the question: {context}  Question: {user_query}`\n",
      "\n",
      "Then the LLM uses the most relevant, diverse, and reranked content to generate the final answer.\n",
      "\n",
      "‚û°Ô∏è **This is the full RAG Fusion pipeline**.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ Option 2: Just return the ranked documents (for inspection or manual use)\n",
      "\n",
      "If your application only needs to show the **most relevant sources**, you might stop here and just display these documents directly to the user (e.g., in a search interface).\n",
      "---\n",
      "\n",
      "### üîç Key Clarification: Decomposition vs Query Expansion\n",
      "\n",
      "Your question:\n",
      "\n",
      "> So it's like in RAG Fusion we generate multiple queries from a single query and in decomposition we break down the query into parts to get results?\n",
      "\n",
      "Yes ‚Äî but here's how to think about their **distinct purposes**:\n",
      "\n",
      "|Aspect|**Query Expansion (RAG Fusion)**|**Decomposition (Query Decomposition)**|\n",
      "|---|---|---|\n",
      "|Goal|Explore **different phrasings or semantic angles** of the same query|Split a **complex query into multiple logically dependent sub-questions**|\n",
      "|Output Queries|Synonyms, paraphrases, or related interpretations|Sub-parts that may need independent answers|\n",
      "|Example Input|\"impact of climate change\"|\"How did climate change affect agriculture and migration?\"|\n",
      "|Generated Queries|\"effect of climate change\", \"global warming consequences\", etc.|1. \"How did climate change affect agriculture?\" 2. \"How did it affect migration?\"|\n",
      "|Used In|**RAG Fusion**, where redundancy is helpful|**Multi-hop QA** or structured reasoning pipelines|\n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ So your summarized logic is:\n",
      "\n",
      "> **RAG Fusion** improves coverage by exploring multiple rewordings (query expansion).  \n",
      "> **Decomposition** handles complexity by breaking a multi-part query into simpler parts.\n",
      "\n",
      "Perfect.\n",
      "\n",
      "Let me know if you‚Äôd like a diagram that compares the two visually.\n",
      "\n",
      "### What is \"Step-Back Prompting\" in RAG?\n",
      "\n",
      "**Step-back prompting** is a technique where, instead of immediately answering a complex or specific question, the system **rephrases the question into a more general or foundational version** that might be easier to retrieve relevant knowledge for. This increases the chance of getting more informative and accurate context from the retriever.\n",
      "\n",
      "---\n",
      "\n",
      "### üß† Why Do This in RAG?\n",
      "\n",
      "RAG works by:\n",
      "\n",
      "1. Using a question to **retrieve relevant documents** from a knowledge base.\n",
      "    \n",
      "2. Feeding both the question and the documents to a language model to generate an answer.\n",
      "    \n",
      "\n",
      "But if the question is too specific or uses uncommon phrasing, the retriever may fail to fetch useful documents.\n",
      "\n",
      "**Step-back prompting improves retrieval** by:\n",
      "\n",
      "- Asking: _\"What is a simpler or more general version of this question?\"_\n",
      "    \n",
      "- Then using that generalized version as a second way to retrieve context.\n",
      "    \n",
      "\n",
      "This gives the language model **two sets of context**:\n",
      "\n",
      "- One from the **original question**.\n",
      "    \n",
      "- One from the **step-back (rephrased) question**.\n",
      "    \n",
      "\n",
      "---\n",
      "\n",
      "### üì¶ How Your Code Uses It\n",
      "\n",
      "Let‚Äôs walk through your pipeline:\n",
      "\n",
      "#### 1. **Few-shot examples**\n",
      "\n",
      "You define how a specific question can be rephrased into a more general one:\n",
      "\n",
      "python\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`\"Could the members of The Police perform lawful arrests?\" ‚Üí \"what can the members of The Police do?\"`\n",
      "\n",
      "This teaches the LLM to generate step-back versions of new questions.\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. **Prompt creation**\n",
      "\n",
      "You create a `FewShotChatMessagePromptTemplate` that uses those examples to help the model learn the pattern of rephrasing.\n",
      "\n",
      "---\n",
      "\n",
      "#### 3. **Step-back question generation**\n",
      "\n",
      "You run this prompt on the new question:\n",
      "\n",
      "python\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`\"What is task decomposition for LLM agents?\" ‚Üí (LLM generates something like) \"what is task decomposition?\"`\n",
      "\n",
      "---\n",
      "\n",
      "#### 4. **Dual-context retrieval**\n",
      "\n",
      "You retrieve context from the vector store or database twice:\n",
      "\n",
      "- Once using the **original question**.\n",
      "    \n",
      "- Once using the **step-back question**.\n",
      "    \n",
      "\n",
      "This ensures both **specific** and **general** information are available to the model.\n",
      "\n",
      "---\n",
      "\n",
      "#### 5. **Final answering**\n",
      "\n",
      "The final prompt includes:\n",
      "\n",
      "- The `normal_context` (from original question),\n",
      "    \n",
      "- The `step_back_context` (from rephrased question),\n",
      "    \n",
      "- The question itself.\n",
      "    \n",
      "\n",
      "And it asks the LLM to give a comprehensive answer using that combined context.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ Summary\n",
      "\n",
      "Step-back prompting in RAG means:\n",
      "\n",
      "> Rewriting a question in a more general or foundational way to improve retrieval quality, and combining that general context with the specific context to generate better answers.\n",
      "\n",
      "## **Original Question:**\n",
      "\n",
      "**\"What is task decomposition for LLM agents?\"**\n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ **STEP 1: HyDE ‚Äî Generate a Hypothetical Passage**\n",
      "\n",
      "Instead of using the question directly to search for documents, we first ask an LLM to **write a fake answer** to this question ‚Äî like a paragraph from a scientific paper.\n",
      "\n",
      "#### üí¨ Prompt given to LLM:\n",
      "\n",
      "text\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`Please write a scientific paper passage to answer the question   Question: What is task decomposition for LLM agents?   Passage:`\n",
      "\n",
      "#### ‚úÖ Output from LLM (Hypothetical Passage):\n",
      "\n",
      "text\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`Task decomposition for LLM agents refers to the process of breaking down complex tasks into smaller, manageable subtasks. This allows the agent to perform step-by-step reasoning and achieve better performance in solving complex problems. It is often used in multi-step reasoning, planning, and decision-making tasks to enhance efficiency and accuracy.`\n",
      "\n",
      "This passage is **not retrieved from the internet**. It's **hallucinated by the LLM** to reflect what a typical answer might look like.\n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ **STEP 2: Use the Hypothetical Passage to Retrieve Real Documents**\n",
      "\n",
      "We now take the **hypothetical passage** (not the question) and use it to search a document database using a retriever (like a vector store).\n",
      "\n",
      "Why?  \n",
      "Because this passage has **keywords and concepts** (like \"step-by-step reasoning\", \"multi-step reasoning\", etc.) that are **more detailed** than the question. This helps retrieve better, more relevant documents.\n",
      "\n",
      "#### üîç Input to the retriever:\n",
      "\n",
      "text\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`Task decomposition for LLM agents refers to the process of breaking down complex tasks into smaller, manageable subtasks...`\n",
      "\n",
      "#### ‚úÖ Output: Retrieved Documents (examples)\n",
      "\n",
      "1. A document explaining chain-of-thought prompting.\n",
      "    \n",
      "2. A paper on how LLMs perform multi-step planning.\n",
      "    \n",
      "3. A tutorial on hierarchical task planning in AI agents.\n",
      "    \n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ **STEP 3: Use Retrieved Documents + Original Question to Answer**\n",
      "\n",
      "Now we take:\n",
      "\n",
      "- ‚úÖ The documents we just retrieved\n",
      "    \n",
      "- ‚úÖ The original question: \"What is task decomposition for LLM agents?\"\n",
      "    \n",
      "\n",
      "And feed them to the LLM again to produce a final answer.\n",
      "\n",
      "#### üí¨ Prompt:\n",
      "\n",
      "text\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`Answer the following question based on this context:  [...insert the 3 documents here...]  Question: What is task decomposition for LLM agents?`\n",
      "\n",
      "#### ‚úÖ LLM Final Answer:\n",
      "\n",
      "text\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`Task decomposition for LLM agents is the process of splitting a complex goal into smaller subtasks that are easier to handle. This enables the model to reason more effectively, track progress, and solve each part individually before combining the results. It is often used in reasoning tasks and agent-based planning systems.`\n",
      "\n",
      "---\n",
      "\n",
      "### üß† Summary:\n",
      "\n",
      "| Step         | What Happens                                     | Why It Matters                               |\n",
      "| ------------ | ------------------------------------------------ | -------------------------------------------- |\n",
      "| 1Ô∏è‚É£ HyDE     | Fake answer generated from the question          | Adds rich details to improve document search |\n",
      "| 2Ô∏è‚É£ Retrieve | Use that fake answer to find real documents      | Improves document quality and relevance      |\n",
      "| 3Ô∏è‚É£ RAG      | Use real documents + original question to answer | Gives accurate, grounded response            |\n",
      "**Hypothetical Document Embeddings**¬†(HyDE) Enhance the retrieval in Haystack using HyDE method by generating a mock-up hypothetical document for an initial query.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 4/20: Things left to do in RAG.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\11. RAG\\Things left to do in RAG.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: Things left to do in RAG\n",
      "updated: 2025-05-07 19:24:28Z\n",
      "created: 2025-05-07 19:23:13Z\n",
      "---\n",
      "\n",
      "## **LangGraph** ‚Äî _Graph-based Language Agent Framework_\n",
      "\n",
      "**LangGraph** is an **open-source framework** built on top of LangChain that enables you to build **multi-step, stateful agents** using **graph structures** (nodes and edges).\n",
      "\n",
      "### üîç What it does:\n",
      "\n",
      "- Think of it as a **graph-based execution engine** for language agents, where:\n",
      "    \n",
      "    - **Nodes** = functions or agents (e.g., a retriever, a planner, or an LLM).\n",
      "        \n",
      "    - **Edges** = the logic that decides what to do next based on the output.\n",
      "        \n",
      "- Supports **loops, conditionals, and memory**, making it easy to implement **complex reasoning workflows**.\n",
      "    \n",
      "\n",
      "### üõ†Ô∏è Use Cases:\n",
      "\n",
      "- Building agents that:\n",
      "    \n",
      "    - Can reflect and revise answers.\n",
      "        \n",
      "    - Decompose and replan based on context.\n",
      "        \n",
      "    - Combine tools like retrieval, code, and summarization.\n",
      "        \n",
      "\n",
      "> **LangGraph = LangChain + Graph Execution + State Management**\n",
      "\n",
      "---\n",
      "\n",
      "## üìö **CRAG** ‚Äî _Chain-of-Recall-Augmented Generation_\n",
      "\n",
      "**CRAG** is a **retrieval-enhanced generation approach** that improves upon vanilla RAG by:\n",
      "\n",
      "1. **Iteratively refining queries** using feedback from the model.\n",
      "    \n",
      "2. Keeping **previous generations in context** for better reasoning.\n",
      "    \n",
      "3. Allowing **recall loops**, like asking ‚Äúwhat did I miss?‚Äù and then re-retrieving.\n",
      "    \n",
      "\n",
      "### üîÑ CRAG Pipeline:\n",
      "\n",
      "1. Initial query ‚Üí retrieve ‚Üí generate answer.\n",
      "    \n",
      "2. Model critiques or expands on what it missed.\n",
      "    \n",
      "3. Refined query ‚Üí retrieve again ‚Üí improve answer.\n",
      "    \n",
      "\n",
      "> Think of CRAG as **RAG + self-awareness + iterative refinement**.\n",
      "\n",
      "---\n",
      "\n",
      "## üîÅ **Self-RAG** ‚Äî _Retrieval with Self-Reflection_\n",
      "\n",
      "**Self-RAG** introduces **retrieval strategies guided by the model itself**:\n",
      "\n",
      "- The model generates **queries** to retrieve context and then **reflects** on whether the retrieved content was helpful.\n",
      "    \n",
      "- If not helpful, it generates **better follow-up queries**.\n",
      "    \n",
      "\n",
      "This is similar to **RAG-Fusion**, but with **model-driven feedback loops**.\n",
      "\n",
      "### Key Ideas:\n",
      "\n",
      "- Self-generation of queries.\n",
      "    \n",
      "- Self-assessment of retrieved context.\n",
      "    \n",
      "- Optional re-querying until relevant content is found.\n",
      "    \n",
      "\n",
      "> Self-RAG is essentially a **model-guided, adaptive retrieval process**.\n",
      "\n",
      "---\n",
      "\n",
      "## üß† **Impact of Long Context**\n",
      "\n",
      "Large context windows (e.g., 32k+ tokens) **significantly change the design of RAG systems**:\n",
      "\n",
      "### ‚úÖ Benefits:\n",
      "\n",
      "- You can now **feed more retrieved documents** directly to the model.\n",
      "    \n",
      "- Reduces the need for **complex reranking or summarization** before passing to the LLM.\n",
      "    \n",
      "- Enables **multi-document reasoning**, like comparing multiple sources.\n",
      "    \n",
      "\n",
      "### ‚ùóChallenges:\n",
      "\n",
      "- More content ‚â† better answers ‚Äî quality still depends on **relevance**.\n",
      "    \n",
      "- **Latency** and **token cost** increase with longer inputs.\n",
      "    \n",
      "- The model might still **hallucinate** if irrelevant context dominates.\n",
      "    \n",
      "\n",
      "### Trend:\n",
      "\n",
      "Long context helps, but **selective retrieval and reranking** (e.g., with Cohere or Self-RAG) are **still crucial** for maintaining performance.\n",
      "\n",
      "---\n",
      "\n",
      "## üß© Summary Table\n",
      "\n",
      "|Term|Description|\n",
      "|---|---|\n",
      "|**LangGraph**|Framework for building stateful, graph-based language agents|\n",
      "|**CRAG**|Retrieval + generation pipeline with self-critique and iterative recall|\n",
      "|**Self-RAG**|Model reflects on retrieved context and adapts retrieval accordingly|\n",
      "|**Long Context**|Allows inputting more data to LLMs, but selective relevance still matters|\n",
      "https://www.youtube.com/watch?v=E2shqsYwxck&ab_channel=LangChain\n",
      "https://www.youtube.com/watch?v=SsHUNfhF32s&ab_channel=LangChain\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 5/20: Technology and the intution.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\13. Agentic AI\\Technology and the intution.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: Technology and the intution\n",
      "updated: 2025-05-08 17:41:35Z\n",
      "created: 2025-05-08 17:34:56Z\n",
      "---\n",
      "\n",
      "Langflow\n",
      "Langchain \n",
      "Phidata\n",
      "autogen\n",
      "\n",
      "okay these tools used by agentic ai are basically what is mentioned on the website of langchain right , under tools? and if so then how do you actually create an agent? first tell me if agentic ai uses tools or not if yes then how? do we just paas these tools? and let the agent decide? if yes then this could be done in simple rag as well no?\n",
      "\n",
      "###### ChatGPT said:\n",
      "\n",
      "Excellent follow-up. Let's break this down carefully and clearly.\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ **Does Agentic AI use tools?**\n",
      "\n",
      "**Yes**, agentic AI uses tools (also called ‚Äúactions,‚Äù ‚Äúplugins,‚Äù or ‚Äútools‚Äù depending on the framework). In LangChain, **tools** are clearly defined functions or APIs the agent can use.\n",
      "\n",
      "Examples:\n",
      "\n",
      "- A `search_wikipedia()` function\n",
      "    \n",
      "- A `get_weather(location)` function\n",
      "    \n",
      "- A math calculator\n",
      "    \n",
      "- A Python REPL\n",
      "    \n",
      "- A SQL query executor\n",
      "    \n",
      "\n",
      "---\n",
      "\n",
      "## üß† **How does Agentic AI use tools?**\n",
      "\n",
      "Here‚Äôs the key idea:  \n",
      "üëâ You **pass these tools to the agent** when creating it, along with a language model.\n",
      "\n",
      "### In LangChain or similar frameworks:\n",
      "\n",
      "python\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`from langchain.agents import initialize_agent, Tool from langchain.tools import WikipediaAPIWrapper from langchain.chat_models import ChatOpenAI  # Define tools wiki_tool = Tool(     name=\"Wikipedia Search\",     func=WikipediaAPIWrapper().run,     description=\"Search Wikipedia for facts\" )  # Pass tools to agent llm = ChatOpenAI(temperature=0) agent = initialize_agent(     tools=[wiki_tool],      llm=llm,      agent=\"zero-shot-react-description\" )`\n",
      "\n",
      "Now, when you ask:\n",
      "\n",
      "> ‚ÄúWho was the first person to climb Everest?‚Äù\n",
      "\n",
      "The agent reasons:\n",
      "\n",
      "- ‚ÄúThis requires a Wikipedia search.‚Äù\n",
      "    \n",
      "- Calls `wiki_tool`\n",
      "    \n",
      "- Gets info and responds.\n",
      "    \n",
      "\n",
      "---\n",
      "\n",
      "## ü§î **Isn‚Äôt this possible with RAG too?**\n",
      "\n",
      "Sort of, **but not dynamically**.\n",
      "\n",
      "### In RAG:\n",
      "\n",
      "- You **manually** retrieve context from Wikipedia (via a vector DB or API).\n",
      "    \n",
      "- Then pass that to the model:\n",
      "    \n",
      "\n",
      "python\n",
      "\n",
      "CopyEdit\n",
      "\n",
      "`context = wiki_api.search(\"First Everest climber\") prompt = f\"Answer using this info: {context}\" response = llm(prompt)`\n",
      "\n",
      "You're doing the thinking for the model:\n",
      "\n",
      "- Which source to use\n",
      "    \n",
      "- How to use it\n",
      "    \n",
      "- When to use it\n",
      "    \n",
      "\n",
      "---\n",
      "\n",
      "## üß† **Agentic AI vs. RAG: Key Difference**\n",
      "\n",
      "|Feature|RAG (Retrieval-Augmented Generation)|Agentic AI|\n",
      "|---|---|---|\n",
      "|Tool Usage|Manual/integrated|Dynamic and AI-controlled|\n",
      "|Reasoning|You define flow|AI decides what tools to use|\n",
      "|Scaling to many tools|Complex|Easier with proper setup|\n",
      "|Decision-making|Static|Goal-directed + flexible|\n",
      "\n",
      "---\n",
      "\n",
      "## ‚úÖ So, to summarize:\n",
      "\n",
      "- Yes, **agentic AI uses tools**.\n",
      "    \n",
      "- You **pass the tools** into the agent when initializing it.\n",
      "    \n",
      "- The **agent decides when and how to use them** based on your query.\n",
      "    \n",
      "- **RAG can use external sources**, but the **logic is handcrafted**.\n",
      "    \n",
      "- Agentic AI **offloads orchestration to the AI**, enabling more autonomy.\n",
      "\n",
      "![[Pasted image 20250508231135.png]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 6/20: Types.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\13. Agentic AI\\Types.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: Types\n",
      "updated: 2025-05-08 19:18:10Z\n",
      "created: 2025-05-08 18:19:56Z\n",
      "---\n",
      "\n",
      "### What is an AI Agent?\n",
      "\n",
      "An **AI agent** is a system that perceives its environment through **sensors** and acts upon that environment through **actuators** to achieve specific goals.\n",
      "\n",
      "---\n",
      "\n",
      "### üß† 1. Simple Reflex Agent\n",
      "\n",
      "- **Behavior**: Reacts only to the _current_ situation using **if-condition-then-action** rules.\n",
      "    \n",
      "- **No memory or learning**.\n",
      "    \n",
      "- **Example**: Thermostat ‚Äî \"If temp < 18¬∞C, turn heater on.\"\n",
      "    \n",
      "\n",
      "‚úÖ Fast  \n",
      "‚ùå Can't handle complex or unpredictable environments.\n",
      "\n",
      "---\n",
      "\n",
      "### üß† 2. Model-Based Reflex Agent\n",
      "\n",
      "- **Adds memory**: Tracks internal **state** of the world based on past perceptions.\n",
      "    \n",
      "- **Still uses condition-action rules**, but with more context.\n",
      "    \n",
      "- **Example**: Robot vacuum ‚Äî remembers cleaned areas, not just what it sees now.\n",
      "    \n",
      "\n",
      "‚úÖ Smarter reactions  \n",
      "‚ùå Still lacks long-term planning or goal pursuit\n",
      "\n",
      "---\n",
      "\n",
      "### üß† 3. Goal-Based Agent\n",
      "\n",
      "- **Focuses on goals**, not just reactions.\n",
      "    \n",
      "- **Uses its internal model** to **simulate outcomes** of actions and choose the best one to reach its goal.\n",
      "    \n",
      "- **Example**: Self-driving car ‚Äî \"I want to reach destination X. Should I turn left?\"\n",
      "    \n",
      "\n",
      "‚úÖ Plans to meet objectives  \n",
      "‚ùå Needs accurate modeling of the environment\n",
      "\n",
      "---\n",
      "\n",
      "### üß† 4. Utility-Based Agent\n",
      "\n",
      "- **Beyond goals**, it evaluates **how good** each outcome is using a **utility function** (like a happiness or desirability score).\n",
      "    \n",
      "- **Chooses best outcome**, not just one that ‚Äúmeets the goal.‚Äù\n",
      "    \n",
      "- **Example**: Drone ‚Äî delivers package in the safest, most energy-efficient way.\n",
      "    \n",
      "\n",
      "‚úÖ Can compare outcomes  \n",
      "‚ùå Requires designing a good utility function\n",
      "\n",
      "---\n",
      "\n",
      "### üß† 5. Learning Agent (Most Advanced)\n",
      "\n",
      "- **Learns from experience** and **improves over time**.\n",
      "    \n",
      "- Has four parts:\n",
      "    \n",
      "    - **Learning Element**: Updates knowledge from feedback.\n",
      "        \n",
      "    - **Performance Element**: Chooses actions.\n",
      "        \n",
      "    - **Critic**: Gives feedback (reward or error).\n",
      "        \n",
      "    - **Problem Generator**: Suggests new actions to explore.\n",
      "        \n",
      "- **Example**: AI Chess bot ‚Äî learns strategies from thousands of games.\n",
      "    \n",
      "\n",
      "‚úÖ Most powerful and flexible  \n",
      "‚ùå Slow and data-intensive\n",
      "\n",
      "---\n",
      "\n",
      "### ü§ù Bonus: Multi-Agent Systems\n",
      "\n",
      "- Multiple agents working **together** in the same environment.\n",
      "    \n",
      "- Can **cooperate** toward shared goals.\n",
      "    \n",
      "- Examples: Swarms of drones, AI teammates in video games.\n",
      "    \n",
      "\n",
      "---\n",
      "\n",
      "### üß≠ Summary Chart\n",
      "\n",
      "| Agent Type         | Memory? | Goals? | Utility? | Learns? |\n",
      "| ------------------ | ------- | ------ | -------- | ------- |\n",
      "| Simple Reflex      | ‚ùå       | ‚ùå      | ‚ùå        | ‚ùå       |\n",
      "| Model-Based Reflex | ‚úÖ       | ‚ùå      | ‚ùå        | ‚ùå       |\n",
      "| Goal-Based         | ‚úÖ       | ‚úÖ      | ‚ùå        | ‚ùå       |\n",
      "| Utility-Based      | ‚úÖ       | ‚úÖ      | ‚úÖ        | ‚ùå       |\n",
      "| Learning Agent     | ‚úÖ       | ‚úÖ      | ‚úÖ        | ‚úÖ       |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 7/20: Google Books to Read.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\15. Holiday Studying GenAI\\Google Books to Read.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: Google Books to Read\n",
      "updated: 2025-06-20 07:49:21Z\n",
      "created: 2025-06-20 07:48:18Z\n",
      "---\n",
      "\n",
      "![[22365_3_Prompt Engineering_v7 (1).pdf]]\n",
      "\n",
      "![[22365_13_Solving Domain-Specific problems using LLMs_v7.pdf]]\n",
      "\n",
      "![[22365_14_Operationalizing Generative AI on Vertex AI_v7 (1).pdf]]\n",
      "\n",
      "![[22365_19_Agents_v8.pdf]]\n",
      "\n",
      "![[whitepaper_emebddings_vectorstores_v2.pdf]]\n",
      "\n",
      "![[whitepaper_Foundational Large Language models & text generation_v2.pdf]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 8/20: TO READ CHROME TABS.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\15. Holiday Studying GenAI\\TO READ CHROME TABS.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: TO READ CHROME TABS\n",
      "updated: 2025-06-26 05:46:44Z\n",
      "created: 2025-06-26 05:45:40Z\n",
      "---\n",
      "\n",
      "https://www.sciencedirect.com/science/article/pii/S1566253523001148#sec13\n",
      "https://ieeexplore.ieee.org/abstract/document/7029673?casa_token=ASON2cQH4-UAAAAA:J08odDNcgv1KubdRWW5W11wx-RPZ_cZ0sQ0NfXazuSVZUHm-2IL9NJiln4y9jjBXdZTab-rYzFY\n",
      "https://www.sciencedirect.com/science/article/abs/pii/S1566253520303651\n",
      "https://web-and-mobile-development.medium.com/how-to-create-an-ai-chatbot-a-comprehensive-guide-to-building-intelligent-conversational-agents-af7eb5d27804\n",
      "https://www.anthropic.com/engineering/building-effective-agents\n",
      "https://docs.llamaindex.ai/en/stable/getting_started/async_python/\n",
      "https://arxiv.org/abs/2410.03160\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 9/20: Agents.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\Agents.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: Agents\n",
      "updated: 2025-06-06 04:19:50Z\n",
      "created: 2025-06-06 03:37:20Z\n",
      "---\n",
      "\n",
      "Agents are AI framework which have an access to the environment and is capable of reasoning , planning and execution of actions through tools to achieve its task. And observe the output too in some cases.\n",
      "\n",
      "Here LLM is the brain and the actions and tools are the body of an agent.\n",
      "\n",
      "---\n",
      "## Tokens\n",
      "\n",
      "Special tokens are use to tell the llm that this is the start or the of the sentence so stop generating from here. Each LLM has some¬†**special tokens**¬†specific to the model.\n",
      "\n",
      "---\n",
      "### System Messages\n",
      "\n",
      "System messages (also called System Prompts) define¬†**how the model should behave**. They serve as¬†**persistent instructions**, guiding every subsequent interaction.\n",
      "\n",
      "system message = {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are a professional customer service agent. Always be polite, clear, and helpful.\"\n",
      "}\n",
      "\n",
      "---\n",
      "## Chat template\n",
      "\n",
      "Chat templates help maintain context by preserving conversation history, storing previous exchanges between the user and the assistant. This leads to more coherent multi-turn conversations.\n",
      "\n",
      "conversation = [\n",
      "    {\"role\": \"user\", \"content\": \"I need help with my order\"},\n",
      "    {\"role\": \"assistant\", \"content\": \"I'd be happy to help. Could you provide your order number?\"},\n",
      "    {\"role\": \"user\", \"content\": \"It's ORDER-123\"},\n",
      "]\n",
      "\n",
      "As we just explained, we always concatenate all the messages in the conversation and pass it to the LLM as a single stand-alone sequence. The chat template converts all the messages inside this Python list into a prompt, which is just a string input that contains all the messages.\n",
      "\n",
      "For example, this is how the SmolLM2 chat template would format the previous exchange into a prompt:\n",
      "\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "I need help with my order<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'd be happy to help. Could you provide your order number?<|im_end|>\n",
      "<|im_start|>user\n",
      "It's ORDER-123<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "#### There are methods to keep your llm in context of all the things which have been talked about in a particular chat-->\n",
      "\n",
      "##### 1. **Summarization of Previous Turns**\n",
      "\n",
      "- Instead of passing _all_ old messages verbatim, you pass a **summary** of earlier context.\n",
      "    \n",
      "- Only the **most recent few turns** are passed in full.\n",
      "\n",
      "##### 2. **Retrieval-Augmented Memory (like RAG)**\n",
      "\n",
      "- Store prior conversations in a **vector database or memory module**.\n",
      "    \n",
      "- For every new user message, **retrieve only the most relevant past exchanges**.\n",
      "    \n",
      "- Add those to the prompt dynamically.\n",
      "\n",
      "##### 3. **Windowed Context (Sliding Window)**\n",
      "\n",
      "- Pass only the **last N messages** (e.g., last 6‚Äì8 turns).\n",
      "    \n",
      "- Accept that some older information will be forgotten.\n",
      "\n",
      "##### 4. **Long-context Models**\n",
      "\n",
      "- Use models with larger context windows (like Claude 3 or GPT-4 Turbo).\n",
      "    \n",
      "- These can handle **entire books or long dialogues** without summarizing.\n",
      "\n",
      "---\n",
      "## Core\n",
      "\n",
      "So agents have the ability to think ie plan , execute and observes their actions . the first thing is planning and breaking down what to do in simpler steps , then this plan is executed and some tools are used here so actions are performed and now according to the system prompt , ie the rule you gave your llm , its gonna evaluate weather the output is valid or sufficient or not and iterate over the same cycle again. -- thought -- action -- observe. Till it sorta finds the optimal answer.\n",
      "\n",
      "---\n",
      "## The ReAct Approach\n",
      "\n",
      "A key method is the¬†**ReAct approach**, which is the concatenation of ‚ÄúReasoning‚Äù (Think) with ‚ÄúActing‚Äù (Act).ReAct is a simple prompting technique that appends ‚ÄúLet‚Äôs think step by step‚Äù before letting the LLM decode the next tokens.These models have been trained to always include specific¬†_thinking_¬†sections (enclosed between¬†`<think>`¬†and¬†`</think>`¬†special tokens)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 10/20: TO--DO.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\TO--DO.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: TO--DO\n",
      "updated: 2025-06-06 07:09:22Z\n",
      "created: 2025-06-05 10:08:36Z\n",
      "---\n",
      "\n",
      "https://huggingface.co/learn/cookbook/en/rag_evaluation\n",
      "\n",
      "https://huggingface.co/learn/cookbook/en/advanced_rag\n",
      "\n",
      "https://huggingface.co/papers/2306.05685\n",
      "\n",
      "https://huggingface.co/papers/2312.10003\n",
      "\n",
      "https://huggingface.co/learn\n",
      "\n",
      "https://huggingface.co/learn/mcp-course/unit0/introduction\n",
      "\n",
      "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\n",
      "\n",
      "https://www.youtube.com/watch?v=x2GRE-RzmD8&ab_channel=IBMTechnology\n",
      "\n",
      "https://www.redhat.com/en/topics/ai/what-is-vllm\n",
      "\n",
      "https://huggingface.co/learn/cookbook/multiagent_rag_system\n",
      "\n",
      "https://huggingface.co/docs/smolagents/main/en/examples/multiagents\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 11/20: Intro.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\Langraphs\\Intro.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: Intro\n",
      "updated: 2025-06-20 19:29:35Z\n",
      "created: 2025-06-20 09:55:50Z\n",
      "---\n",
      "\n",
      "## When should I use LangGraph ?\n",
      "\n",
      "### [](https://huggingface.co/learn/agents-course/unit2/langgraph/when_to_use_langgraph#control-vs-freedom)Control vs freedom\n",
      "\n",
      "When designing AI applications, you face a fundamental trade-off between¬†**control**¬†and¬†**freedom**:\n",
      "\n",
      "- **Freedom**¬†gives your LLM more room to be creative and tackle unexpected problems.\n",
      "- **Control**¬†allows you to ensure predictable behavior and maintain guardrails.\n",
      "\n",
      "Code Agents, like the ones you can encounter in¬†_smolagents_, are very free. They can call multiple tools in a single action step, create their own tools, etc. However, this behavior can make them less predictable and less controllable than a regular Agent working with JSON!\n",
      "\n",
      "`LangGraph`¬†is on the other end of the spectrum, it shines when you need¬†**‚ÄúControl‚Äù**¬†on the execution of your agent.\n",
      "\n",
      "LangGraph is particularly valuable when you need¬†**Control over your applications**. It gives you the tools to build an application that follows a predictable process while still leveraging the power of LLMs.\n",
      "\n",
      "Put simply, if your application involves a series of steps that need to be orchestrated in a specific way, with decisions being made at each junction point,¬†**LangGraph provides the structure you need**.\n",
      "\n",
      "The key scenarios where LangGraph excels include:\n",
      "\n",
      "- **Multi-step reasoning processes**¬†that need explicit control on the flow\n",
      "- **Applications requiring persistence of state**¬†between steps\n",
      "- **Systems that combine deterministic logic with AI capabilities**\n",
      "- **Workflows that need human-in-the-loop interventions**\n",
      "- **Complex agent architectures**¬†with multiple components working together\n",
      "\n",
      "In essence, whenever possible,¬†**as a human**, design a flow of actions based on the output of each action, and decide what to execute next accordingly. In this case, LangGraph is the correct framework for you!\n",
      "\n",
      "\t`LangGraph`¬†is, in my opinion, the most production-ready agent framework on the market.\n",
      "\n",
      "h 2 3 nj 8 10 \n",
      "\n",
      "what about orlando from 31 to 2nd in pdf?\n",
      "and where in new jersey?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 12/20: Intro.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\MCP\\Intro.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: Intro\n",
      "updated: 2025-07-26 16:49:21Z\n",
      "created: 2025-07-26 16:38:57Z\n",
      "---\n",
      "\n",
      "The AI ecosystem is evolving rapidly, with Large Language Models (LLMs) and other AI systems becoming increasingly capable. However, these models are often limited by their training data and lack access to real-time information or specialized tools. This limitation hinders the potential of AI systems to provide truly relevant, accurate, and helpful responses in many scenarios.\n",
      "\n",
      "This is where Model Context Protocol (MCP) comes in. MCP enables AI models to connect with external data sources, tools, and environments, allowing for the seamless transfer of information and capabilities between AI systems and the broader digital world\n",
      "\n",
      "¬†**M√óN Integration Problem**\n",
      "- **Host**: The user-facing AI application that end-users interact with directly. Examples include Anthropic‚Äôs Claude Desktop, AI-enhanced IDEs like Cursor, inference libraries like Hugging Face Python SDK, or custom applications built in libraries like LangChain or smolagents. Hosts initiate connections to MCP Servers and orchestrate the overall flow between user requests, LLM processing, and external tools.\n",
      "    \n",
      "- **Client**: A component within the host application that manages communication with a specific MCP Server. Each Client maintains a 1:1 connection with a single Server, handling the protocol-level details of MCP communication and acting as an intermediary between the Host‚Äôs logic and the external Server.\n",
      "    \n",
      "- **Server**: An external program or service that exposes capabilities (Tools, Resources, Prompts) via the MCP protocol.\n",
      "## Resources\n",
      "\n",
      "Resources provide read-only access to data sources, allowing the AI model to retrieve context without executing complex logic.\n",
      "\n",
      "## Sampling\n",
      "\n",
      "Sampling allows Servers to request the Client (specifically, the Host application) to perform LLM interactions.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 13/20: QueryEngine.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\Summary LlamaIndex\\QueryEngine.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: QueryEngine\n",
      "updated: 2025-06-20 08:12:09Z\n",
      "created: 2025-06-06 10:42:12Z\n",
      "---\n",
      "\n",
      "Traditional RAG with new terms and annotations\n",
      "NODE -- is a piece of chunk\n",
      "\n",
      "---\n",
      "https://docs.llamaindex.ai/en/stable/#introduction\n",
      "---\n",
      "## Evaluation Metric\n",
      "\n",
      "- `FaithfulnessEvaluator`: Evaluates the faithfulness of the answer by checking if the answer is supported by the context.\n",
      "- `AnswerRelevancyEvaluator`: Evaluate the relevance of the answer by checking if the answer is relevant to the question.\n",
      "- `CorrectnessEvaluator`: Evaluate the correctness of the answer by checking if the answer is correct.\n",
      "\n",
      "LlamaTrace For evaluation of the Agent performance\n",
      "\n",
      "---\n",
      "## Tools Here\n",
      "\n",
      "1. `FunctionTool`: Convert any Python function into a tool that an agent can use. It automatically figures out how the function works.\n",
      "2. `QueryEngineTool`: A tool that lets agents use query engines. Since agents are built on query engines, they can also use other agents as tools.\n",
      "3. `Toolspecs`: Sets of tools created by the community, which often include tools for specific services like Gmail.\n",
      "4. `Utility Tools`: Special tools that help handle large amounts of data from other tools.\n",
      "\n",
      "---\n",
      "## ToolSpecs\n",
      "\n",
      "Think of¬†`ToolSpecs`¬†as collections of tools that work together harmoniously - like a well-organized professional toolkit. Just as a mechanic‚Äôs toolkit contains complementary tools that work together for vehicle repairs, a¬†`ToolSpec`¬†combines related tools for specific purposes. For example, an accounting agent‚Äôs¬†`ToolSpec`¬†might elegantly integrate spreadsheet capabilities, email functionality, and calculation tools to handle financial tasks with precision and efficiency\n",
      "\n",
      "---\n",
      "## Utility Tools\n",
      "\n",
      "Oftentimes, directly querying an API¬†**can return an excessive amount of data**, some of which may be irrelevant, overflow the context window of the LLM, or unnecessarily increase the number of tokens that you are using. Let‚Äôs walk through our two main utility tools below.\n",
      "\n",
      "1. `OnDemandToolLoader`: This tool turns any existing LlamaIndex data loader (BaseReader class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger¬†`load_data`¬†from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it ‚Äòon-demand‚Äô. All three of these steps happen in a single tool call.\n",
      "2. `LoadAndSearchToolSpec`: The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements¬†`to_tool_list`, and when that function is called, two tools are returned: a loading tool and then a search tool. The load Tool execution would call the underlying Tool, and then index the output (by default with a vector index). The search Tool execution would take in a query string as input and call the underlying index.\n",
      "\n",
      "This is the code example of how to implement llamaindex into your agentic workflow\n",
      "https://colab.research.google.com/#fileId=https%3A//huggingface.co/agents-course/notebooks/blob/main/unit2/llama-index/workflows.ipynb\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 14/20: Smolagent.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\Summary SmolAgent\\Smolagent.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: Smolagent\n",
      "updated: 2025-06-06 06:08:48Z\n",
      "created: 2025-06-05 19:38:01Z\n",
      "---\n",
      "\n",
      "> Basically i think their should be a summary of each stuff which i learn cause , have to revise as well and do not want to forget these things which is why I'm gonna do this , starting here , leave the rest behind and lets focus on this.\n",
      "> \n",
      "   https://huggingface.co/docs/smolagents/index -- the documentation\n",
      "---\n",
      "### CodeAgent\n",
      "- Is a multi step agent which breaks down task into smaller pieces and then perform them by writing code and executing it. It also debugs any errors if they appear in the output so the observation and iteration is still here. \n",
      "- Code is run in a sandbox ie \"Sandboxed execution for security\" means running code in a tightly controlled environment to prevent it from causing harm.\n",
      "- Primary task to write code and execute it till the answer is optimal and all , which is what we see here , when calculating the time taking to start and finish the preparation of a party. It could regenerate the code again and again , sorta gradient descent.\n",
      "---\n",
      "### Code\n",
      "\n",
      "Now we can use the agent to access tools , be it custom or inbuilt so that is what we see here in the next section. To plan a party decide a menu and songs.\n",
      "\n",
      "---\n",
      "### Push/Pull\n",
      "\n",
      "We can even push and pull agents and use them from time to time , code is present in the documentation.\n",
      "\n",
      "---\n",
      "### Inspecting Our Party Preparator Agent with OpenTelemetry and Langfuse\n",
      "\n",
      "the¬†[OpenTelemetry](https://opentelemetry.io/)¬†standard for instrumenting agent runs, allowing seamless inspection and logging. With the help of¬†[Langfuse](https://langfuse.com/)¬†and the¬†`SmolagentsInstrumentor`, Alfred can easily track and analyze his agent‚Äôs behavior.\n",
      "\n",
      "---\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 15/20: VisionTransformers.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\16. HuggingFace\\Summary SmolAgent\\VisionTransformers.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: VisionTransformers\n",
      "updated: 2025-06-06 09:51:48Z\n",
      "created: 2025-06-06 09:48:58Z\n",
      "---\n",
      "\n",
      "- [We just gave sight to smolagents](https://huggingface.co/blog/smolagents-can-see)¬†- Blog describing the vision agent functionality.\n",
      "- [Web Browser Automation with Agents ü§ñüåê](https://huggingface.co/docs/smolagents/examples/web_browser)¬†- Example for Web browsing using a vision agent.\n",
      "- [Web Browser Vision Agent Example](https://github.com/huggingface/smolagents/blob/main/src/smolagents/vision_web_browser.py)¬†- Example for Web browsing using a vision agent.\\\n",
      "---\n",
      "Here we see how an agent can validate weather a person is present in the database or not , by using selenium to search browser and storing the image there in local storage along with generating a caption of it to match the attire of the person present , so yes a great job to do here.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 16/20: RAG+.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\19. Summary Of Research Paper\\RAG+.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: RAG+\n",
      "updated: 2025-06-30 09:14:00Z\n",
      "created: 2025-06-30 07:15:42Z\n",
      "---\n",
      "\n",
      " ![[2506.11555v1.pdf]]\n",
      "\n",
      "## Terms in the research paper to study\n",
      "1. RA-DIT (Lin et al., 2023) fine-tunes the retriever and generator in a dual-instruction manner, align- ing retrieval more closely with what the model needs to generate accurate response.\n",
      "2.  Re-TASK (Wang et al., 2024b) introduces the concept of capability items, emphasizing that accomplishing domain-specific tasks requires jointly leveraging domain knowledge and task-specific skills.\n",
      "3. Rewrite-Retrieve-Read (Ma et al., 2023) rewrites queries to better match doc- ument style.\n",
      "4. R2AG (Ye et al., 2024) uses a retrieval-aware encoder to highlight key signals, while Query2Doc (Wang et al., 2023a) expands queries into pseudo-documents to clarify intent. On the corpus side,\n",
      "5.  GraphRAG (Edge et al., 2024) integrates knowledge graphs to support entity- centric reasoning\n",
      "6.  To enhance retrieval precision, Reranking (Wang et al., 2024a) and filtering tech- niques (Wang et al., 2023b; Pickett et al., 2025) further refine retrieved content.\n",
      "7.  Agentic RAG (Singh et al., 2025)\n",
      "8. RAG-Gym (Xiong et al., 2025) decompose tasks into subtasks han- dled by specialized agents or workflow\n",
      "9. OPEN- RAG (Islam et al., 2024) introduces agent-based de- composition and context selection\n",
      "10. , while RAT (Zi- hao et al., 2024) uses Chain-of-Thought prompting to support step-wise retrieval.\n",
      "11. LLMs with temperature sampling and self-consistency voting\n",
      "12. Answer-First RAG (AFRAG) first generates a candidate answer from the query, which is then used to retrieve supporting evidence.\n",
      "\n",
      "\n",
      "# Summary Of the Paper\n",
      "\n",
      "So what happens here in RAG+ is that you would have this knowledge corpus which is gonna contain the domain knowledge about the subject and all , the fact and theorems and all , next we would have an application corpus as well where we will have the application of each of this knowledge database corpus , this is a many to many mapping over here so when we query with the pipeline , we get both the knowledge and is corresponding application of the query and with this context our llm generates the final answer. The knowledge is preprepared prior to the pipeline and the application domain is generated with the help of an llm through semi supervised learning i guess.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 17/20: When was in Artemis.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\2. Chrome\\When was in Artemis.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: When was in Artemis\n",
      "updated: 2025-07-30 16:41:35Z\n",
      "created: 2025-07-30 16:38:14Z\n",
      "---\n",
      "\n",
      " https://www.youtube.com/@financialforensics8886/streams\n",
      " https://www.youtube.com/watch?v=TjHH_--7l8g&ab_channel=Serrano.Academy\n",
      " https://arxiv.org/pdf/2506.16411\n",
      " https://arxiv.org/pdf/2507.02554\n",
      " https://arxiv.org/pdf/2503.09600\n",
      " https://arxiv.org/pdf/2310.05002\n",
      " https://github.com/google-gemini/gemini-cli/blob/main/docs/index.md \n",
      " https://www.youtube.com/watch?v=CqL5kB8pOfo&ab_channel=aiwithbrandon\n",
      " https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb\n",
      " https://console.groq.com/docs/models\n",
      " https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-tracing\n",
      " https://transformer-circuits.pub/2025/attribution-graphs/methods.html\n",
      " https://huggingface.co/blog/tiny-agents\n",
      " https://www.continue.dev/about-us\n",
      " https://www.cloudskillsboost.google/course_templates/1081?catalog_rank=%7B%22rank%22%3A8%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&search_id=48843221\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 18/20: 2025-05-28.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\20. Stuff-I-Don_t-Need\\2025-05-28.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: 2025-05-28\n",
      "updated: 2025-05-28 16:22:10Z\n",
      "created: 2025-05-28 16:13:34Z\n",
      "---\n",
      "\n",
      "Todo:\n",
      "1. Flowbit == 25k\n",
      "\tObjective\n",
      "\tBuild a multi-agent AI system that accepts input in PDF, JSON, or Email (text) format, classifies the format and intent, and routes it to the appropriate agent. The system must maintain shared context (e.g., sender, topic, last extracted fields) to enable chaining and traceability.\n",
      "\t\n",
      "\tSystem Overview\n",
      "\tYou will build 3 agents, orchestrated via a central Classifier Agent:\n",
      "\t\n",
      "\t1. Classifier Agent\n",
      "\tInput: raw file/email/JSON\n",
      "\tClassifies:\n",
      "\tFormat: PDF / JSON / Email\n",
      "\tIntent: Invoice, RFQ, Complaint, Regulation, etc.\n",
      "\tRoutes to correct agent\n",
      "\tLogs format + intent in memory\n",
      "\t\n",
      "\t2. JSON Agent\n",
      "\tAccepts structured JSON payloads\n",
      "\tExtracts/reformats to a target schema\n",
      "\tFlags anomalies or missing fields\n",
      "\t\n",
      "\t3. Email Agent\n",
      "\tAccepts email content\n",
      "\tExtracts sender, intent, urgency\n",
      "\tFormats for CRM-style usage\n",
      "\t\n",
      "\tShared Memory Module (Lightweight)\n",
      "\tStores:\n",
      "\tSource, type, timestamp\n",
      "\tExtracted values\n",
      "\tThread or conversation ID\n",
      "\tCan be Redis, SQLite, or in-memory\n",
      "\tMust be accessible across agents\n",
      "\t\n",
      "\t Example Flow\n",
      "\tUser sends email ‚Üí Classifier detects ‚ÄúEmail + RFQ‚Äù ‚Üí Routed to Email Agent ‚Üí Extracts info ‚Üí Memory logs result\n",
      "\t\n",
      "\tTech Stack Suggestions\n",
      "\tPython\n",
      "\tLLMs (OpenAI or open-source)\n",
      "\tRedis / SQLite / JSON store for memory\n",
      "\t\n",
      "\tSubmission Requirements\n",
      "\tWorking video demo (mandatory)\n",
      "\tGitHub repo with:\n",
      "\tREADME.md\n",
      "\tSample input files\n",
      "\tFolder structure\n",
      "\tSample output logs or screenshots\n",
      "\t\n",
      "\tSend your submissions to soham.shah@flowbitai.com\n",
      "\t\n",
      "\tThanks,\n",
      "\tSoham\n",
      "\t\n",
      "\tSubmission deadline: 01 June, 2025\n",
      "\t\n",
      "\tSubmit assignment\n",
      "\n",
      "2. Accredian\n",
      "\n",
      "Dear Candidate,\n",
      "\n",
      "Thank you for applying at Accredian.\n",
      "As a screening round, you have to complete the given assignment.\n",
      "\n",
      "Assignment Link: [https://drive.google.com/uc?id=1BiTEaQ6MM3OXku8EhDoCa9EGhHmIuCGM&export=download](https://drive.google.com/uc?id=1BiTEaQ6MM3OXku8EhDoCa9EGhHmIuCGM&export=download)\n",
      "\n",
      "Once finished with the assignment task, you have to fill a Google form.\n",
      "\n",
      "Submission Requirements: Updated Resume (at max of 2 Pages), and Jupyter Notebook.\n",
      "\n",
      "Google Form Link: [https://forms.gle/F3AcN3LeK8uJRSvG6](https://forms.gle/F3AcN3LeK8uJRSvG6)\n",
      "\n",
      "Deadline to submit: 29th May  2025 till 11:59 PM.\n",
      "\n",
      "Candidates who will submit the task in time will be considered qualified for the next round.\n",
      "\n",
      "Regards,\n",
      "Mrinali Jaiswal\n",
      "Senior Associate- Talent Acquisition\n",
      "HR | ACCREDIAN\n",
      "M: (+91) 8851121470\n",
      "E: mrinali.jaiswal@accredian.com\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 19/20: 1. Intro.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\4. MCP\\1. Intro.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: 1. Intro\n",
      "updated: 2025-07-15 06:10:31Z\n",
      "created: 2025-07-14 07:49:58Z\n",
      "---\n",
      "\n",
      "MCP is a framework through which developers can embedded context and meaning to the model without explicitly mentioning what tool and which model to use. The MCP server takes care of that. You just declare what needs to happen instead of coding manually.\n",
      "\n",
      "MCP (Model Context Protocol) is a framework that lets developers describe what context (like tools, documents, memory, or functions) a model should use‚Äîwithout manually coding how to connect them. Instead of wiring everything together, you just declare what you need, and the system sets it up so the model can use it intelligently.\n",
      "\n",
      "MCP lets you describe what a model should have access to‚Äîlike tools or memory‚Äîwithout writing all the code to wire it up. The platform handles that for you.\n",
      "\n",
      "Model Context Protocol (MCP) is an open standard developed by Anthropic, the company behind Claude. While it may sound technical, but the core idea is simple:¬†give AI agents a consistent way to connect with tools, services, and data ‚Äî no matter where they live or how they‚Äôre built.\n",
      "\n",
      "![[Pasted image 20250714150825.png]]\n",
      "\n",
      "Client - is what allows us to access some functionality which is present inside the MCP server\n",
      "Resources in MCP allow your server to expose information that can be directly included in prompts, rather than requiring tool calls to access data. This creates a more efficient way to provide context to AI models.\n",
      "\n",
      "![[Pasted image 20250714163303.png]]\n",
      "\n",
      "## Choosing the Right Primitive\n",
      "\n",
      "Here's a quick decision guide:\n",
      "\n",
      "- **Need to give Claude new capabilities?**¬†Use tools\n",
      "- **Need to get data into your app for UI or context?**¬†Use resources\n",
      "- **Want to create predefined workflows for users?**¬†Use prompt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "üìñ FILE 20/20: chatbot with message summarizer.md\n",
      "üìÇ Path: D:\\LOST.DIR\\Obsidian Vault\\6. LangGraphs\\module-2\\chatbot with message summarizer.md\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---\n",
      "title: chatbot with message summarizer\n",
      "updated: 2025-07-18 20:25:25Z\n",
      "created: 2025-07-18 18:57:32Z\n",
      "---\n",
      "\n",
      "Trim and filter message which the agents sends to the llm its that shit , so how do we revoke the accessibility of the last 2 messages so those are remove from the state and how to trim with remove module as well in Stategraph i guess and last one is restricting the no. of tokens to some limit so that's all for that video i suppose.'\n",
      "this video shows the integration of all we have learned so far\n",
      "and the last video of the module has this threat to talk about which is passed during invoke of each message and how to connect to the external database thats all their is to it here.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "‚úÖ Finished viewing all selected files\n",
      "\n",
      "================================================================================\n",
      "üîç ENHANCED KEYWORD FILE FINDER\n",
      "‚ú® Features: Multi-select ‚Ä¢ View multiple files ‚Ä¢ Spell checking\n",
      "================================================================================\n",
      "‚ùå Please enter a keyword\n",
      "\n",
      "================================================================================\n",
      "üîç ENHANCED KEYWORD FILE FINDER\n",
      "‚ú® Features: Multi-select ‚Ä¢ View multiple files ‚Ä¢ Spell checking\n",
      "================================================================================\n",
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List\n",
    "import difflib\n",
    "\n",
    "class EnhancedKeywordFinder:\n",
    "    def __init__(self, vault_path):\n",
    "        self.vault_path = Path(vault_path)\n",
    "        print(f\"üìÅ Vault path: {vault_path}\")\n",
    "        \n",
    "        # Load all files and extract common words for spell checking\n",
    "        self.all_files = list(self.vault_path.rglob(\"*.md\"))\n",
    "        self.common_words = set()\n",
    "        self._build_vocabulary()\n",
    "        \n",
    "    def _build_vocabulary(self):\n",
    "        \"\"\"Build vocabulary from all files for spell checking\"\"\"\n",
    "        print(\"üîß Building vocabulary for spell checking...\")\n",
    "        for file_path in self.all_files[:50]:  # Sample first 50 files for vocabulary\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read().lower()\n",
    "                    # Extract words (3+ characters)\n",
    "                    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', content)\n",
    "                    self.common_words.update(words)\n",
    "            except:\n",
    "                continue\n",
    "        print(f\"‚úÖ Built vocabulary with {len(self.common_words)} words\")\n",
    "    \n",
    "    def spell_check_keyword(self, keyword):\n",
    "        \"\"\"Try to correct spelling of keyword\"\"\"\n",
    "        if not self.common_words:\n",
    "            return keyword\n",
    "            \n",
    "        # Get close matches\n",
    "        suggestions = difflib.get_close_matches(\n",
    "            keyword.lower(), \n",
    "            self.common_words, \n",
    "            n=3, \n",
    "            cutoff=0.6\n",
    "        )\n",
    "        \n",
    "        if suggestions and suggestions[0] != keyword.lower():\n",
    "            print(f\"üî§ Did you mean: {', '.join(suggestions[:3])}?\")\n",
    "            return suggestions[0]\n",
    "        \n",
    "        return keyword\n",
    "    \n",
    "    def search_keyword(self, keyword, auto_correct=True):\n",
    "        \"\"\"Search for keyword with optional spell correction\"\"\"\n",
    "        original_keyword = keyword\n",
    "        print(f\"\\nüîç Searching for keyword: '{keyword}'\")\n",
    "        \n",
    "        if not self.all_files:\n",
    "            print(\"‚ùå No .md files found in vault!\")\n",
    "            return []\n",
    "        \n",
    "        matching_files = []\n",
    "        processed = 0\n",
    "        \n",
    "        for file_path in self.all_files:\n",
    "            processed += 1\n",
    "            if processed % 100 == 0:\n",
    "                print(f\"   Processed {processed}/{len(self.all_files)} files...\")\n",
    "                \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                    # Case-insensitive substring search\n",
    "                    if keyword.lower() in content.lower():\n",
    "                        context = self.get_keyword_context(content, keyword)\n",
    "                        matching_files.append({\n",
    "                            'file_path': str(file_path),\n",
    "                            'file_name': file_path.name,\n",
    "                            'folder': str(file_path.parent),\n",
    "                            'context': context,\n",
    "                            'file_size': len(content)\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # If no matches and auto-correct enabled, try spell correction\n",
    "        if not matching_files and auto_correct:\n",
    "            corrected_keyword = self.spell_check_keyword(keyword)\n",
    "            if corrected_keyword != keyword.lower():\n",
    "                print(f\"üîÑ Trying corrected spelling: '{corrected_keyword}'\")\n",
    "                return self.search_keyword(corrected_keyword, auto_correct=False)\n",
    "        \n",
    "        if matching_files:\n",
    "            print(f\"‚úÖ Found {len(matching_files)} files containing '{keyword}'\")\n",
    "        else:\n",
    "            print(f\"‚ùå No files found containing '{keyword}'\")\n",
    "            \n",
    "        return matching_files\n",
    "    \n",
    "    def get_keyword_context(self, content, keyword):\n",
    "        \"\"\"Get text around the keyword for preview\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        context_lines = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if keyword.lower() in line.lower():\n",
    "                # Get surrounding lines for context\n",
    "                start = max(0, i-1)\n",
    "                end = min(len(lines), i+2)\n",
    "                context = '\\n'.join(lines[start:end])\n",
    "                context_lines.append(context)\n",
    "                \n",
    "                if len(context_lines) >= 2:  # Limit context snippets\n",
    "                    break\n",
    "        \n",
    "        return '\\n...\\n'.join(context_lines) if context_lines else \"No context found\"\n",
    "    \n",
    "    def display_search_results(self, matching_files):\n",
    "        \"\"\"Display search results with selection options\"\"\"\n",
    "        if not matching_files:\n",
    "            return\n",
    "            \n",
    "        print(f\"\\nüìã FOUND {len(matching_files)} FILES:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, file_info in enumerate(matching_files, 1):\n",
    "            print(f\"\\n{i}. {file_info['file_name']}\")\n",
    "            print(f\"   üìÇ Folder: {file_info['folder']}\")\n",
    "            print(f\"   üìè Size: {file_info['file_size']:,} chars\")\n",
    "            print(f\"   üìù Context Preview:\")\n",
    "            \n",
    "            # Show context with limited lines\n",
    "            context_lines = file_info['context'].split('\\n')[:3]\n",
    "            for line in context_lines:\n",
    "                print(f\"      {line[:100]}{'...' if len(line) > 100 else ''}\")\n",
    "    \n",
    "    def select_multiple_files(self, matching_files):\n",
    "        \"\"\"Allow user to select multiple files\"\"\"\n",
    "        if not matching_files:\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nüéØ SELECT FILES (Multiple Selection Supported):\")\n",
    "        print(\"Examples:\")\n",
    "        print(\"  ‚Ä¢ Single: 1\")\n",
    "        print(\"  ‚Ä¢ Multiple: 1,3,5\")\n",
    "        print(\"  ‚Ä¢ Range: 1-3\")\n",
    "        print(\"  ‚Ä¢ All: all\")\n",
    "        \n",
    "        while True:\n",
    "            choice = input(f\"\\nYour selection (1-{len(matching_files)}): \").strip()\n",
    "            \n",
    "            if not choice:\n",
    "                print(\"‚ùå Please make a selection\")\n",
    "                continue\n",
    "            \n",
    "            if choice.lower() == 'all':\n",
    "                return matching_files\n",
    "            \n",
    "            try:\n",
    "                selected_indices = []\n",
    "                \n",
    "                # Handle comma-separated values\n",
    "                parts = choice.split(',')\n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    \n",
    "                    # Handle ranges (e.g., \"1-3\")\n",
    "                    if '-' in part:\n",
    "                        start, end = map(int, part.split('-'))\n",
    "                        selected_indices.extend(range(start-1, end))\n",
    "                    else:\n",
    "                        selected_indices.append(int(part) - 1)\n",
    "                \n",
    "                # Filter valid indices\n",
    "                valid_indices = [i for i in selected_indices if 0 <= i < len(matching_files)]\n",
    "                \n",
    "                if not valid_indices:\n",
    "                    print(\"‚ùå No valid selections made\")\n",
    "                    continue\n",
    "                \n",
    "                selected_files = [matching_files[i] for i in valid_indices]\n",
    "                print(f\"‚úÖ Selected {len(selected_files)} files\")\n",
    "                return selected_files\n",
    "                \n",
    "            except ValueError:\n",
    "                print(\"‚ùå Invalid format. Use numbers, commas, or ranges (e.g., 1,3,5 or 1-3)\")\n",
    "                continue\n",
    "    \n",
    "    def view_selected_files(self, selected_files):\n",
    "        \"\"\"Display content of selected files\"\"\"\n",
    "        if not selected_files:\n",
    "            print(\"‚ùå No files selected\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüìö VIEWING {len(selected_files)} SELECTED FILES:\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        for i, file_info in enumerate(selected_files, 1):\n",
    "            print(f\"\\nüìñ FILE {i}/{len(selected_files)}: {file_info['file_name']}\")\n",
    "            print(f\"üìÇ Path: {file_info['file_path']}\")\n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            try:\n",
    "                with open(file_info['file_path'], 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    print(content)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error reading file: {e}\")\n",
    "            \n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            # Pause between files for large selections\n",
    "            if i < len(selected_files):\n",
    "                input(f\"\\nPress Enter to view next file ({i+1}/{len(selected_files)})...\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Finished viewing all selected files\")\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Main interactive search interface\"\"\"\n",
    "        while True:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"üîç ENHANCED KEYWORD FILE FINDER\")\n",
    "            print(\"‚ú® Features: Multi-select ‚Ä¢ View multiple files ‚Ä¢ Spell checking\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            keyword = input(\"\\nüí≠ Enter keyword to search: \").strip()\n",
    "            \n",
    "            if not keyword:\n",
    "                print(\"‚ùå Please enter a keyword\")\n",
    "                continue\n",
    "                \n",
    "            if keyword.lower() in ['quit', 'exit', 'stop', 'q']:\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Search for files\n",
    "            matching_files = self.search_keyword(keyword)\n",
    "            \n",
    "            if not matching_files:\n",
    "                continue\n",
    "            \n",
    "            # Display results\n",
    "            self.display_search_results(matching_files)\n",
    "            \n",
    "            # Let user select multiple files\n",
    "            selected_files = self.select_multiple_files(matching_files)\n",
    "            \n",
    "            if not selected_files:\n",
    "                continue\n",
    "            \n",
    "            # Ask what to do with selected files\n",
    "            print(f\"\\nüéØ What would you like to do with {len(selected_files)} selected files?\")\n",
    "            print(\"1. View file contents\")\n",
    "            print(\"2. Search for different keyword\")\n",
    "            print(\"3. Quit\")\n",
    "            \n",
    "            action = input(\"Choose action (1-3): \").strip()\n",
    "            \n",
    "            if action == '1':\n",
    "                self.view_selected_files(selected_files)\n",
    "            elif action == '2':\n",
    "                continue\n",
    "            elif action == '3':\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"‚ùå Invalid choice, returning to search...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß† Enhanced Multi-Select Keyword Finder\")\n",
    "    print(\"Perfect for large Joplin vaults with spell checking!\")\n",
    "    \n",
    "    vault_path = input(\"\\nEnter path to your vault: \").strip()\n",
    "    \n",
    "    if not vault_path:\n",
    "        print(\"‚ùå Please provide a valid path\")\n",
    "        exit()\n",
    "    \n",
    "    if not Path(vault_path).exists():\n",
    "        print(f\"‚ùå Path {vault_path} doesn't exist\")\n",
    "        exit()\n",
    "    \n",
    "    finder = EnhancedKeywordFinder(vault_path)\n",
    "    finder.interactive_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec98899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
