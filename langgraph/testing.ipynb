{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f96bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def process_with_groq_clip(state):\n",
    "    # Get API key from environment\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    \n",
    "    # Debug: Check if API key is loaded\n",
    "    if not api_key:\n",
    "        print(\"‚ùå GROQ_API_KEY not found in environment variables\")\n",
    "        return {**state, \"llm_response\": \"API key not configured\"}\n",
    "    \n",
    "    print(f\"‚úÖ API key loaded: {api_key[:6]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80412ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß FIXED: CLIP-Enhanced Document Processor with Groq\n",
      "--------------------------------------------------\n",
      "üìã Instructions:\n",
      "1. Get your API key from: https://console.groq.com/keys\n",
      "2. Replace 'gsk_your_actual_groq_api_key_here' in the code\n",
      "3. Or set environment variable: export GROQ_API_KEY=your_key\n",
      "üìÅ Loading documents from: E:\\7. Projects From Sem 3\\RAG\\data\\rag_1_Intro_20250906_032403\n",
      "‚úÖ Loaded 1 markdown files and 4 images\n",
      "üé® Generating CLIP embeddings for 4 images\n",
      "‚úÖ Processed: Pasted image 20250714150825.png\n",
      "‚úÖ Processed: Pasted image 20250714163303.png\n",
      "‚úÖ Processed: Pasted image 20250714150825.png\n",
      "‚úÖ Processed: Pasted image 20250714163303.png\n",
      "üìù Generating CLIP text embeddings for 1 texts\n",
      "üîó Generated 4 image embeddings and 1 text embeddings\n",
      "‚úÖ Using Groq API key: gsk_ST...M0ri\n",
      "‚úÖ Groq client initialized successfully\n",
      "üîÑ Calling Groq API...\n",
      "üîç Response type: <class 'groq.types.chat.chat_completion.ChatCompletion'>\n",
      "üîç Response object: ChatCompletion(id='chatcmpl-67f8e1ac-f697-4bc1-853a-1f256dce37cc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"### Comprehensive Summary\\n\\nThe provided markdown document discusses the Model Context Protocol (MCP), a framework developed by Anthropic that enables developers to embed context and meaning into AI models without explicitly coding the connections. MCP allows developers to describe what context a model should use, such as tools, documents, memory, or functions, and the system sets it up for intelligent use. The framework includes components like the MCP server, client, resources, and tools, which work together to provide a consistent way for AI agents to connect with services and data. The document also provides guidance on choosing the right primitive (tools, resources, or prompts) based on specific needs, such as giving Claude new capabilities, getting data into an app, or creating predefined workflows.\\n\\n### Key Insights from Semantic Alignment\\n\\nThe semantic similarity score of 0.261 between the text and images suggests a moderate level of alignment between the two modalities. Key insights from this alignment include:\\n\\n1. **Conceptual Understanding**: The images likely depict abstract concepts related to MCP, such as connections, frameworks, or AI models, which are also discussed in the text.\\n2. **Visual Representation**: The images may visually represent the relationships between components like the MCP server, client, resources, and tools, which are described in the text.\\n3. **Illustrative Examples**: The images could be providing illustrative examples of how MCP works, such as diagrams or flowcharts, to support the textual explanations.\\n\\n### Visual Themes and Patterns\\n\\nThrough CLIP analysis, the following visual themes and patterns can be identified:\\n\\n1. **Connectivity and Networks**: Images may depict connections, nodes, or networks, representing the relationships between MCP components and AI models.\\n2. **Abstract Concepts**: Visualizations of abstract concepts, such as context, meaning, or intelligence, may be present in the images.\\n3. **Technical Diagrams**: Images could include technical diagrams, flowcharts, or architecture diagrams, illustrating the inner workings of MCP and its components.\\n\\n### Specific Questions for Spaced Repetition Learning\\n\\nTo combine text and visual understanding, consider the following questions for spaced repetition learning:\\n\\n1. How does the MCP server facilitate connections between AI models and context?\\n2. What is the role of resources in MCP, and how do they enhance the efficiency of providing context to AI models?\\n3. Can you describe a scenario where using tools in MCP would be more appropriate than using resources or prompts?\\n4. How does the client component in MCP enable access to functionality within the MCP server?\\n5. What are the benefits of using MCP for developers, and how does it simplify the process of embedding context and meaning into AI models?\\n\\n### Enhancement of Textual Content by Visual Elements\\n\\nThe visual elements enhance the textual content in several ways:\\n\\n1. **Illustrative Examples**: Images provide concrete examples of abstract concepts, making it easier for readers to understand complex ideas.\\n2. **Visual Representation**: Visualizations of relationships between components and AI models help readers grasp the architecture and inner workings of MCP.\\n3. **Engagement and Attention**: The inclusion of images breaks up the text, making the content more engaging and attention-grabbing, which can improve comprehension and retention.\\n4. **Supplementary Information**: Images may convey additional information not explicitly stated in the text, such as the scale, complexity, or dynamics of MCP components, which can further enrich the reader's understanding.\", role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1757277868, model='llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ddfbb0da0', usage=CompletionUsage(completion_tokens=681, prompt_tokens=670, total_tokens=1351, completion_time=1.8135386549999999, prompt_time=0.056485372, queue_time=0.046362698, total_time=1.870024027), usage_breakdown=None, x_groq={'id': 'req_01k4jyrjnnfm5tchrgwv79xtt6'}, service_tier='on_demand')\n",
      "ü§ñ Groq processing with CLIP embeddings completed successfully!\n",
      "\n",
      "============================================================\n",
      "üéâ PROCESSING COMPLETE!\n",
      "============================================================\n",
      "### Comprehensive Summary\n",
      "\n",
      "The provided markdown document discusses the Model Context Protocol (MCP), a framework developed by Anthropic that enables developers to embed context and meaning into AI models without explicitly coding the connections. MCP allows developers to describe what context a model should use, such as tools, documents, memory, or functions, and the system sets it up for intelligent use. The framework includes components like the MCP server, client, resources, and tools, which work together to provide a consistent way for AI agents to connect with services and data. The document also provides guidance on choosing the right primitive (tools, resources, or prompts) based on specific needs, such as giving Claude new capabilities, getting data into an app, or creating predefined workflows.\n",
      "\n",
      "### Key Insights from Semantic Alignment\n",
      "\n",
      "The semantic similarity score of 0.261 between the text and images suggests a moderate level of alignment between the two modalities. Key insights from this alignment include:\n",
      "\n",
      "1. **Conceptual Understanding**: The images likely depict abstract concepts related to MCP, such as connections, frameworks, or AI models, which are also discussed in the text.\n",
      "2. **Visual Representation**: The images may visually represent the relationships between components like the MCP server, client, resources, and tools, which are described in the text.\n",
      "3. **Illustrative Examples**: The images could be providing illustrative examples of how MCP works, such as diagrams or flowcharts, to support the textual explanations.\n",
      "\n",
      "### Visual Themes and Patterns\n",
      "\n",
      "Through CLIP analysis, the following visual themes and patterns can be identified:\n",
      "\n",
      "1. **Connectivity and Networks**: Images may depict connections, nodes, or networks, representing the relationships between MCP components and AI models.\n",
      "2. **Abstract Concepts**: Visualizations of abstract concepts, such as context, meaning, or intelligence, may be present in the images.\n",
      "3. **Technical Diagrams**: Images could include technical diagrams, flowcharts, or architecture diagrams, illustrating the inner workings of MCP and its components.\n",
      "\n",
      "### Specific Questions for Spaced Repetition Learning\n",
      "\n",
      "To combine text and visual understanding, consider the following questions for spaced repetition learning:\n",
      "\n",
      "1. How does the MCP server facilitate connections between AI models and context?\n",
      "2. What is the role of resources in MCP, and how do they enhance the efficiency of providing context to AI models?\n",
      "3. Can you describe a scenario where using tools in MCP would be more appropriate than using resources or prompts?\n",
      "4. How does the client component in MCP enable access to functionality within the MCP server?\n",
      "5. What are the benefits of using MCP for developers, and how does it simplify the process of embedding context and meaning into AI models?\n",
      "\n",
      "### Enhancement of Textual Content by Visual Elements\n",
      "\n",
      "The visual elements enhance the textual content in several ways:\n",
      "\n",
      "1. **Illustrative Examples**: Images provide concrete examples of abstract concepts, making it easier for readers to understand complex ideas.\n",
      "2. **Visual Representation**: Visualizations of relationships between components and AI models help readers grasp the architecture and inner workings of MCP.\n",
      "3. **Engagement and Attention**: The inclusion of images breaks up the text, making the content more engaging and attention-grabbing, which can improve comprehension and retention.\n",
      "4. **Supplementary Information**: Images may convey additional information not explicitly stated in the text, such as the scale, complexity, or dynamics of MCP components, which can further enrich the reader's understanding.\n",
      "        \n",
      "        --- CLIP EMBEDDING ANALYSIS ---\n",
      "        ‚Ä¢ Image embeddings: 4 vectors (512D each)\n",
      "        ‚Ä¢ Text embeddings: 1 vectors (512D each)  \n",
      "        ‚Ä¢ Ready for multimodal question generation and spaced repetition!\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Dict, Any, List\n",
    "import os\n",
    "import markdown\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional: Load from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"dotenv not installed. Using os.environ directly.\")\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    folder_path: str\n",
    "    documents: List[Dict[str, Any]]\n",
    "    image_paths: List[str]\n",
    "    image_embeddings: List[List[float]]\n",
    "    text_embeddings: List[List[float]]\n",
    "    processed_content: str\n",
    "    llm_response: str\n",
    "\n",
    "# Your existing helper functions remain the same...\n",
    "def extract_image_paths(md_text: str, folder_path: str) -> List[str]:\n",
    "    img_pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    relative_paths = re.findall(img_pattern, md_text)\n",
    "    absolute_paths = []\n",
    "    \n",
    "    for img_path in relative_paths:\n",
    "        if not os.path.isabs(img_path):\n",
    "            abs_path = os.path.join(folder_path, img_path)\n",
    "            if os.path.exists(abs_path):\n",
    "                absolute_paths.append(abs_path)\n",
    "        else:\n",
    "            if os.path.exists(img_path):\n",
    "                absolute_paths.append(img_path)\n",
    "    \n",
    "    return absolute_paths\n",
    "\n",
    "def get_clip_embeddings(image_paths: List[str]) -> List[List[float]]:\n",
    "    if not image_paths:\n",
    "        return []\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"üé® Generating CLIP embeddings for {len(image_paths)} images\")\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = model.encode_image(image)\n",
    "                embedding = embedding.cpu().numpy().flatten().tolist()\n",
    "            embeddings.append(embedding)\n",
    "            print(f\"‚úÖ Processed: {os.path.basename(img_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing image {img_path}: {str(e)}\")\n",
    "            embeddings.append([0.0] * 512)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def get_text_clip_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"üìù Generating CLIP text embeddings for {len(texts)} texts\")\n",
    "    \n",
    "    for text in texts:\n",
    "        try:\n",
    "            text_tokens = clip.tokenize([text[:77]]).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = model.encode_text(text_tokens)\n",
    "                embedding = embedding.cpu().numpy().flatten().tolist()\n",
    "            embeddings.append(embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing text: {str(e)}\")\n",
    "            embeddings.append([0.0] * 512)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Your existing load_documents and generate_clip_embeddings functions...\n",
    "def load_documents(state: GraphState) -> GraphState:\n",
    "    folder_path = state[\"folder_path\"]\n",
    "    \n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"Invalid folder path: {folder_path}\")\n",
    "    \n",
    "    documents = []\n",
    "    all_image_paths = []\n",
    "    \n",
    "    print(f\"üìÅ Loading documents from: {folder_path}\")\n",
    "    \n",
    "    for file_path in Path(folder_path).glob(\"*.md\"):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            md_content = f.read()\n",
    "            \n",
    "        html_content = markdown.markdown(md_content)\n",
    "        image_paths = extract_image_paths(md_content, folder_path)\n",
    "        all_image_paths.extend(image_paths)\n",
    "        \n",
    "        documents.append({\n",
    "            \"filename\": file_path.name,\n",
    "            \"markdown\": md_content,\n",
    "            \"html\": html_content,\n",
    "            \"images\": image_paths,\n",
    "            \"word_count\": len(md_content.split())\n",
    "        })\n",
    "    \n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}\n",
    "    for file_path in Path(folder_path).rglob(\"*\"):\n",
    "        if file_path.suffix.lower() in image_extensions:\n",
    "            abs_path = str(file_path.absolute())\n",
    "            if abs_path not in all_image_paths:\n",
    "                all_image_paths.append(abs_path)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} markdown files and {len(all_image_paths)} images\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"documents\": documents,\n",
    "        \"image_paths\": all_image_paths\n",
    "    }\n",
    "\n",
    "def generate_clip_embeddings(state: GraphState) -> GraphState:\n",
    "    documents = state[\"documents\"]\n",
    "    image_paths = state[\"image_paths\"]\n",
    "    \n",
    "    image_embeddings = get_clip_embeddings(image_paths)\n",
    "    texts = [doc[\"markdown\"] for doc in documents]\n",
    "    text_embeddings = get_text_clip_embeddings(texts)\n",
    "    \n",
    "    print(f\"üîó Generated {len(image_embeddings)} image embeddings and {len(text_embeddings)} text embeddings\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"image_embeddings\": image_embeddings,\n",
    "        \"text_embeddings\": text_embeddings\n",
    "    }\n",
    "\n",
    "# FIXED: Groq processing with proper API key handling\n",
    "def process_with_groq_fixed(state: GraphState) -> GraphState:\n",
    "    \"\"\"Process with Groq - FIXED version with proper response handling\"\"\"\n",
    "    \n",
    "    # Get API key\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        api_key = \"gsk_your_actual_groq_api_key_here\"  # Replace with your key\n",
    "    \n",
    "    if not api_key or api_key == \"gsk_your_actual_groq_api_key_here\":\n",
    "        return {\n",
    "            **state,\n",
    "            \"llm_response\": \"‚ùå GROQ API KEY NOT CONFIGURED\"\n",
    "        }\n",
    "    \n",
    "    print(f\"‚úÖ Using Groq API key: {api_key[:6]}...{api_key[-4:]}\")\n",
    "    \n",
    "    try:\n",
    "        client = Groq(api_key=api_key)\n",
    "        print(\"‚úÖ Groq client initialized successfully\")\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            **state,\n",
    "            \"llm_response\": f\"‚ùå Failed to initialize Groq client: {str(e)}\"\n",
    "        }\n",
    "    \n",
    "    # Process your data\n",
    "    documents = state[\"documents\"]\n",
    "    image_paths = state[\"image_paths\"]\n",
    "    image_embeddings = state[\"image_embeddings\"]\n",
    "    text_embeddings = state[\"text_embeddings\"]\n",
    "    \n",
    "    combined_text = \"\\n\\n--- DOCUMENT SEPARATOR ---\\n\\n\".join([\n",
    "        f\"**File: {doc['filename']}**\\n\\n{doc['markdown']}\" \n",
    "        for doc in documents\n",
    "    ])\n",
    "    \n",
    "    # Calculate multimodal insights\n",
    "    similarity_analysis = \"\"\n",
    "    if text_embeddings and image_embeddings:\n",
    "        text_avg = np.mean(text_embeddings, axis=0)\n",
    "        img_avg = np.mean(image_embeddings, axis=0)\n",
    "        similarity = np.dot(text_avg, img_avg) / (\n",
    "            np.linalg.norm(text_avg) * np.linalg.norm(img_avg)\n",
    "        )\n",
    "        similarity_analysis = f\"Text-Image semantic similarity: {similarity:.3f}\"\n",
    "    \n",
    "    analysis_prompt = f\"\"\"\n",
    "    You are an intelligent document analyzer with CLIP-based multimodal understanding.\n",
    "    \n",
    "    I have provided you with:\n",
    "    - {len(documents)} markdown documents with CLIP text embeddings\n",
    "    - {len(image_paths)} images with CLIP visual embeddings\n",
    "    - {similarity_analysis}\n",
    "    \n",
    "    Please analyze this multimodal content and provide:\n",
    "    1. A comprehensive summary incorporating both text and visual elements\n",
    "    2. Key insights from the semantic alignment between text and images  \n",
    "    3. Visual themes and patterns identified through CLIP analysis\n",
    "    4. Specific questions for spaced repetition learning that combine text and visual understanding\n",
    "    5. How the visual elements enhance the textual content\n",
    "    \n",
    "    Text Content:\n",
    "    {combined_text[:3000]}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ Calling Groq API...\")\n",
    "        \n",
    "        # Make the API call\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": analysis_prompt\n",
    "            }],\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            max_tokens=1500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # DEBUG: Print response structure to understand what we're getting\n",
    "        print(f\"üîç Response type: {type(response)}\")\n",
    "        print(f\"üîç Response object: {response}\")\n",
    "        \n",
    "        # FIXED: Handle different response structures\n",
    "        try:\n",
    "            # Method 1: Standard response structure\n",
    "            if hasattr(response, 'choices') and response.choices:\n",
    "                if isinstance(response.choices, list):\n",
    "                    # If choices is a list, get the first element\n",
    "                    first_choice = response.choices[0]\n",
    "                    if hasattr(first_choice, 'message'):\n",
    "                        llm_response = first_choice.message.content\n",
    "                    elif hasattr(first_choice, 'text'):\n",
    "                        llm_response = first_choice.text\n",
    "                    else:\n",
    "                        llm_response = str(first_choice)\n",
    "                else:\n",
    "                    # If choices is not a list, try to access message directly\n",
    "                    llm_response = response.choices.message.content\n",
    "            \n",
    "            # Method 2: Direct response content\n",
    "            elif hasattr(response, 'content'):\n",
    "                llm_response = response.content\n",
    "            \n",
    "            # Method 3: Response is a string\n",
    "            elif isinstance(response, str):\n",
    "                llm_response = response\n",
    "            \n",
    "            # Method 4: Response is a dict\n",
    "            elif isinstance(response, dict):\n",
    "                if 'choices' in response and response['choices']:\n",
    "                    llm_response = response['choices'][0]['message']['content']\n",
    "                elif 'content' in response:\n",
    "                    llm_response = response['content']\n",
    "                else:\n",
    "                    llm_response = str(response)\n",
    "            \n",
    "            # Fallback: Convert to string\n",
    "            else:\n",
    "                llm_response = f\"Unexpected response format: {str(response)}\"\n",
    "                \n",
    "        except Exception as parse_error:\n",
    "            llm_response = f\"\"\"\n",
    "            ‚ùå Error parsing Groq response: {str(parse_error)}\n",
    "            \n",
    "            Raw response type: {type(response)}\n",
    "            Raw response: {str(response)[:500]}...\n",
    "            \n",
    "            This suggests the API response format has changed.\n",
    "            \"\"\"\n",
    "        \n",
    "        # Add embedding statistics\n",
    "        embedding_stats = f\"\"\"\n",
    "        \n",
    "        --- CLIP EMBEDDING ANALYSIS ---\n",
    "        ‚Ä¢ Image embeddings: {len(image_embeddings)} vectors (512D each)\n",
    "        ‚Ä¢ Text embeddings: {len(text_embeddings)} vectors (512D each)  \n",
    "        ‚Ä¢ Ready for multimodal question generation and spaced repetition!\n",
    "        \"\"\"\n",
    "        \n",
    "        llm_response += embedding_stats\n",
    "        \n",
    "        print(\"ü§ñ Groq processing with CLIP embeddings completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        llm_response = f\"\"\"\n",
    "        ‚ùå Groq API Error: {str(e)}\n",
    "        \n",
    "        Your CLIP embeddings were generated successfully:\n",
    "        - {len(image_embeddings)} image embeddings\n",
    "        - {len(text_embeddings)} text embeddings\n",
    "        \n",
    "        This multimodal data is ready for:\n",
    "        ‚Ä¢ Question generation based on semantic similarity\n",
    "        ‚Ä¢ Spaced repetition with visual-text alignment\n",
    "        ‚Ä¢ Cross-modal retrieval for enhanced learning\n",
    "        \n",
    "        You can proceed with building your question-answering system using these embeddings!\n",
    "        \"\"\"\n",
    "        print(f\"‚ùå Groq API error: {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"processed_content\": combined_text,\n",
    "        \"llm_response\": llm_response\n",
    "    }\n",
    "\n",
    "# Create the fixed workflow\n",
    "def create_fixed_groq_workflow():\n",
    "    workflow = StateGraph(GraphState)\n",
    "    \n",
    "    workflow.add_node(\"load_documents\", load_documents)\n",
    "    workflow.add_node(\"generate_clip_embeddings\", generate_clip_embeddings)\n",
    "    workflow.add_node(\"process_groq_fixed\", process_with_groq_fixed)  # Fixed version\n",
    "    \n",
    "    workflow.set_entry_point(\"load_documents\")\n",
    "    workflow.add_edge(\"load_documents\", \"generate_clip_embeddings\")\n",
    "    workflow.add_edge(\"generate_clip_embeddings\", \"process_groq_fixed\")\n",
    "    workflow.add_edge(\"process_groq_fixed\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîß FIXED: CLIP-Enhanced Document Processor with Groq\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get your API key from https://console.groq.com/keys\n",
    "    print(\"üìã Instructions:\")\n",
    "    print(\"1. Get your API key from: https://console.groq.com/keys\")\n",
    "    print(\"2. Replace 'gsk_your_actual_groq_api_key_here' in the code\")\n",
    "    print(\"3. Or set environment variable: export GROQ_API_KEY=your_key\")\n",
    "    \n",
    "    folder_path = input(\"\\nEnter folder path with markdown files and images: \").strip()\n",
    "    if not folder_path:\n",
    "        folder_path = \".\"\n",
    "    \n",
    "    # Use the fixed workflow\n",
    "    workflow = create_fixed_groq_workflow()\n",
    "    \n",
    "    initial_state = GraphState(\n",
    "        folder_path=folder_path,\n",
    "        documents=[],\n",
    "        image_paths=[],\n",
    "        image_embeddings=[],\n",
    "        text_embeddings=[],\n",
    "        processed_content=\"\",\n",
    "        llm_response=\"\"\n",
    "    )\n",
    "    \n",
    "    result = workflow.invoke(initial_state)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(result[\"llm_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfa371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
