{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d5377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def process_with_groq_clip(state):\n",
    "    # Get API key from environment\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    \n",
    "    # Debug: Check if API key is loaded\n",
    "    if not api_key:\n",
    "        print(\"‚ùå GROQ_API_KEY not found in environment variables\")\n",
    "        return {**state, \"llm_response\": \"API key not configured\"}\n",
    "    \n",
    "    print(f\"‚úÖ API key loaded: {api_key[:6]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dae75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Multimodal Q&A with Groq Native API\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üìã Supports: Text + Images or Images only\n",
      "ü§ñ Uses Groq's native multimodal capabilities\n",
      "üìÅ Loading content from: E:\\7. Projects From Sem 3\\RAG\\data\\rag_1_Intro_20250906_032403\n",
      "‚úÖ Found 0 documents and 2 images\n",
      "üñºÔ∏è Processing 2 images for multimodal analysis...\n",
      "‚úÖ Added Pasted image 20250714150825.png\n",
      "‚úÖ Added Pasted image 20250714163303.png\n",
      "\n",
      "ü§î MULTIMODAL QUESTION GENERATED:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "**Question:** \n",
      "\n",
      "How do the MCP Server Primitives (Tools, Resources, and Prompts) interact and contribute to the overall workflow depicted in the first image, particularly in terms of control, usage, and information flow between the User, Our Server, MCP Client, MCP Server, Claude, and Github? \n",
      "\n",
      "**Rationale:** \n",
      "\n",
      "This question requires the test-taker to demonstrate an understanding of both images and how they relate to each other. The first image shows a complex workflow involving multiple entities, while the second image provides a categorization of MCP Server Primitives into Tools, Resources, and Prompts, each with distinct characteristics and use cases. \n",
      "\n",
      "To answer this question correctly, the test-taker must be able to:\n",
      "\n",
      "1. Identify the different components of the MCP Server Primitives and their roles.\n",
      "2. Understand how these primitives are used in the workflow depicted in the first image.\n",
      "3. Analyze how the control, usage, and information flow between the entities in the workflow are influenced\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "Your turn! Please answer the question above:\n",
      "\n",
      "‚úÖ Answer recorded: 99 characters\n",
      "üñºÔ∏è Including 2 images in feedback context...\n",
      "\n",
      "üí≠ MULTIMODAL FEEDBACK:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "## Feedback on User's Answer\n",
      "\n",
      "The user's answer, \"using an http server which lets use easily connect to the tool or the server or the database right?\" touches on the idea of using a server for communication but does not fully address the question. The question asks for a detailed explanation of how MCP Server Primitives (Tools, Resources, and Prompts) interact and contribute to the workflow, including control, usage, and information flow between entities.\n",
      "\n",
      "## Key Points for Improvement:\n",
      "\n",
      "1. **Identify MCP Server Primitives:** The user did not explicitly mention Tools, Resources, and Prompts or their characteristics.\n",
      "2. **Understand Usage in Workflow:** The answer lacks detail on how these primitives are used in the depicted workflow.\n",
      "3. **Analyze Control, Usage, and Information Flow:** The user did not analyze how these aspects are influenced by the primitives.\n",
      "\n",
      "## Suggestions:\n",
      "\n",
      "- Review the definitions and roles of Tools, Resources, and Prompts.\n",
      "- Study the workflow diagram to see how these primitives are utilized.\n",
      "- Consider how control, usage, and information flow are managed among the entities.\n",
      "\n",
      "## Conclusion:\n",
      "\n",
      "The user's answer shows a basic understanding but lacks depth. Further review of the provided images and details will help in providing a more accurate and complete response. Keep working on understanding the workflow and primitives.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üéâ MULTIMODAL SESSION COMPLETE!\n",
      "üìä Summary:\n",
      "  ‚Ä¢ Documents: 0\n",
      "  ‚Ä¢ Images: 2\n",
      "  ‚Ä¢ Multimodal question: ‚úÖ\n",
      "  ‚Ä¢ Answer collected: ‚úÖ\n",
      "  ‚Ä¢ Multimodal feedback: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Dict, Any, List\n",
    "import os\n",
    "import markdown\n",
    "import re\n",
    "from PIL import Image\n",
    "import base64\n",
    "from groq import Groq\n",
    "from pathlib import Path\n",
    "\n",
    "class QuestionState(TypedDict):\n",
    "    folder_path: str\n",
    "    documents: List[Dict[str, Any]]\n",
    "    image_paths: List[str]\n",
    "    question: str\n",
    "    user_answer: str\n",
    "    feedback: str\n",
    "\n",
    "def extract_image_paths(md_text: str, folder_path: str) -> List[str]:\n",
    "    img_pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    relative_paths = re.findall(img_pattern, md_text)\n",
    "    absolute_paths = []\n",
    "    \n",
    "    for img_path in relative_paths:\n",
    "        if not os.path.isabs(img_path):\n",
    "            abs_path = os.path.join(folder_path, img_path)\n",
    "            if os.path.exists(abs_path):\n",
    "                absolute_paths.append(abs_path)\n",
    "        else:\n",
    "            if os.path.exists(img_path):\n",
    "                absolute_paths.append(img_path)\n",
    "    \n",
    "    return absolute_paths\n",
    "\n",
    "def encode_image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"Convert image to base64 for Groq API\"\"\"\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "            \n",
    "        # Get image format\n",
    "        image_format = os.path.splitext(image_path)[1][1:].lower()\n",
    "        if image_format == 'jpg':\n",
    "            image_format = 'jpeg'\n",
    "            \n",
    "        return f\"data:image/{image_format};base64,{encoded_string}\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error encoding image {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Node 1: Load documents and images\n",
    "def load_content(state: QuestionState) -> QuestionState:\n",
    "    folder_path = state[\"folder_path\"]\n",
    "    \n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"Invalid folder path: {folder_path}\")\n",
    "    \n",
    "    documents = []\n",
    "    all_image_paths = []\n",
    "    \n",
    "    print(f\"üìÅ Loading content from: {folder_path}\")\n",
    "    \n",
    "    # Load markdown files (if any exist)\n",
    "    md_files = list(Path(folder_path).glob(\"*.md\"))\n",
    "    for file_path in md_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            md_content = f.read()\n",
    "        \n",
    "        image_paths = extract_image_paths(md_content, folder_path)\n",
    "        all_image_paths.extend(image_paths)\n",
    "        \n",
    "        documents.append({\n",
    "            \"filename\": file_path.name,\n",
    "            \"markdown\": md_content,\n",
    "            \"word_count\": len(md_content.split())\n",
    "        })\n",
    "    \n",
    "    # Find ALL images in folder (regardless of markdown references)\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}\n",
    "    for file_path in Path(folder_path).rglob(\"*\"):\n",
    "        if file_path.suffix.lower() in image_extensions:\n",
    "            abs_path = str(file_path.absolute())\n",
    "            if abs_path not in all_image_paths:\n",
    "                all_image_paths.append(abs_path)\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(documents)} documents and {len(all_image_paths)} images\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"documents\": documents,\n",
    "        \"image_paths\": all_image_paths\n",
    "    }\n",
    "\n",
    "# Node 2: Generate question using Groq's multimodal API\n",
    "def generate_multimodal_question(state: QuestionState) -> QuestionState:\n",
    "    documents = state[\"documents\"]\n",
    "    image_paths = state[\"image_paths\"]\n",
    "    \n",
    "    # Check what content we have\n",
    "    has_text = len(documents) > 0\n",
    "    has_images = len(image_paths) > 0\n",
    "    \n",
    "    if not has_text and not has_images:\n",
    "        question = \"‚ùå No content found. Please add markdown files or images to the folder.\"\n",
    "        return {**state, \"question\": question}\n",
    "    \n",
    "    # Prepare multimodal content for Groq API\n",
    "    content = []\n",
    "    \n",
    "    # Add text content if available\n",
    "    if has_text:\n",
    "        combined_text = \"\\n\\n\".join([doc[\"markdown\"] for doc in documents])\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Text Content:\\n{combined_text[:2000]}\"  # Limit text length\n",
    "        })\n",
    "    \n",
    "    # Add images if available (limit to first 5 images to avoid payload issues)\n",
    "    if has_images:\n",
    "        print(f\"üñºÔ∏è Processing {min(len(image_paths), 5)} images for multimodal analysis...\")\n",
    "        \n",
    "        for img_path in image_paths[:5]:  # Limit to 5 images\n",
    "            base64_image = encode_image_to_base64(img_path)\n",
    "            if base64_image:\n",
    "                content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": base64_image\n",
    "                    }\n",
    "                })\n",
    "                print(f\"‚úÖ Added {os.path.basename(img_path)}\")\n",
    "    \n",
    "    # Add instruction for question generation\n",
    "    if has_text and has_images:\n",
    "        instruction = \"Based on the text content and images provided above, generate ONE thoughtful question that tests understanding of the key concepts. The question should integrate both textual and visual information where relevant.\"\n",
    "    elif has_text:\n",
    "        instruction = \"Based on the text content above, generate ONE thoughtful question that tests understanding of the key concepts.\"\n",
    "    else:\n",
    "        instruction = \"Based on the images provided above, generate ONE thoughtful question that tests understanding of the visual content and concepts shown.\"\n",
    "    \n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": instruction\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }],\n",
    "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Multimodal model\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        question = response.choices[0].message.content.strip()\n",
    "        print(f\"\\nü§î MULTIMODAL QUESTION GENERATED:\")\n",
    "        print(f\"‚îÄ\" * 50)\n",
    "        print(question)\n",
    "        print(f\"‚îÄ\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Smart fallback based on available content\n",
    "        if has_text and has_images:\n",
    "            question = f\"Based on the {len(documents)} documents and {len(image_paths)} images, what are the main concepts and how do the visual elements relate to the text?\"\n",
    "        elif has_text:\n",
    "            question = f\"What are the main topics covered in the {len(documents)} documents?\"\n",
    "        else:\n",
    "            question = f\"Based on the {len(image_paths)} images shown, what do you observe and what concepts do they illustrate?\"\n",
    "        \n",
    "        print(f\"‚ö†Ô∏è Using fallback question due to error: {str(e)}\")\n",
    "        print(f\"Question: {question}\")\n",
    "    \n",
    "    return {**state, \"question\": question}\n",
    "\n",
    "# Node 3: Collect user answer\n",
    "def collect_answer(state: QuestionState) -> QuestionState:\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"\\nYour turn! Please answer the question above:\")\n",
    "    user_answer = input(\"Your answer: \").strip()\n",
    "    \n",
    "    if not user_answer:\n",
    "        user_answer = \"No answer provided.\"\n",
    "    \n",
    "    print(f\"\\n‚úÖ Answer recorded: {len(user_answer)} characters\")\n",
    "    \n",
    "    return {**state, \"user_answer\": user_answer}\n",
    "# Node 4: FIXED - Provide feedback using Groq's multimodal API\n",
    "def provide_multimodal_feedback(state: QuestionState) -> QuestionState:\n",
    "    question = state[\"question\"]\n",
    "    user_answer = state[\"user_answer\"]\n",
    "    documents = state[\"documents\"]\n",
    "    image_paths = state[\"image_paths\"]\n",
    "    \n",
    "    # Build content as a list for multimodal input\n",
    "    content = []\n",
    "    \n",
    "    # Add the question and user's answer as text\n",
    "    feedback_text = f\"\"\"\n",
    "    Question: {question}\n",
    "    \n",
    "    User's Answer: {user_answer}\n",
    "    \n",
    "    Please provide brief, constructive feedback on the user's answer based on the question and available content (text and/or images). Be encouraging but honest about accuracy and completeness.\n",
    "    \"\"\"\n",
    "    \n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": feedback_text\n",
    "    })\n",
    "    \n",
    "    # Add text context if available\n",
    "    if documents:\n",
    "        text_context = \"\\n\".join([doc[\"markdown\"][:500] for doc in documents])\n",
    "        content.append({\n",
    "            \"type\": \"text\", \n",
    "            \"text\": f\"Original Text Context:\\n{text_context}\"\n",
    "        })\n",
    "    \n",
    "    # Add images for context (limit to 3 for feedback)\n",
    "    if image_paths:\n",
    "        print(f\"üñºÔ∏è Including {min(len(image_paths), 3)} images in feedback context...\")\n",
    "        \n",
    "        for img_path in image_paths[:3]:\n",
    "            base64_image = encode_image_to_base64(img_path)\n",
    "            if base64_image:\n",
    "                content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": base64_image\n",
    "                    }\n",
    "                })\n",
    "    \n",
    "    try:\n",
    "        client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        \n",
    "        # FIXED: Proper message format for multimodal content\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": content  # This should be the list directly\n",
    "            }],\n",
    "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Use multimodal model for feedback too\n",
    "            max_tokens=300,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        feedback = response.choices[0].message.content.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        content_summary = f\"{len(documents)} documents and {len(image_paths)} images\"\n",
    "        feedback = f\"Thank you for your answer! Based on the analysis of {content_summary}, your response has been recorded. Due to a technical issue ({str(e)}), detailed feedback is not available right now.\"\n",
    "    \n",
    "    print(f\"\\nüí≠ MULTIMODAL FEEDBACK:\")\n",
    "    print(f\"‚îÄ\" * 50)\n",
    "    print(feedback)\n",
    "    print(f\"‚îÄ\" * 50)\n",
    "    \n",
    "    return {**state, \"feedback\": feedback}\n",
    "\n",
    "\n",
    "# Build the workflow\n",
    "def create_multimodal_qa_workflow():\n",
    "    workflow = StateGraph(QuestionState)\n",
    "    \n",
    "    workflow.add_node(\"load_content\", load_content)\n",
    "    workflow.add_node(\"generate_question\", generate_multimodal_question)\n",
    "    workflow.add_node(\"collect_answer\", collect_answer)\n",
    "    workflow.add_node(\"provide_feedback\", provide_multimodal_feedback)\n",
    "    \n",
    "    workflow.set_entry_point(\"load_content\")\n",
    "    workflow.add_edge(\"load_content\", \"generate_question\")\n",
    "    workflow.add_edge(\"generate_question\", \"collect_answer\")\n",
    "    workflow.add_edge(\"collect_answer\", \"provide_feedback\")\n",
    "    workflow.add_edge(\"provide_feedback\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Main execution\n",
    "def run_multimodal_qa_session():\n",
    "    print(\"üéØ Multimodal Q&A with Groq Native API\")\n",
    "    print(\"‚ïê\" * 45)\n",
    "    print(\"üìã Supports: Text + Images or Images only\")\n",
    "    print(\"ü§ñ Uses Groq's native multimodal capabilities\")\n",
    "    \n",
    "    # Get folder path\n",
    "    folder_path = input(\"\\nEnter folder path with markdown files and/or images: \").strip()\n",
    "    if not folder_path:\n",
    "        folder_path = \".\"\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = QuestionState(\n",
    "        folder_path=folder_path,\n",
    "        documents=[],\n",
    "        image_paths=[],\n",
    "        question=\"\",\n",
    "        user_answer=\"\",\n",
    "        feedback=\"\"\n",
    "    )\n",
    "    \n",
    "    # Run workflow\n",
    "    try:\n",
    "        workflow = create_multimodal_qa_workflow()\n",
    "        result = workflow.invoke(initial_state)\n",
    "        \n",
    "        print(f\"\\nüéâ MULTIMODAL SESSION COMPLETE!\")\n",
    "        print(f\"üìä Summary:\")\n",
    "        print(f\"  ‚Ä¢ Documents: {len(result['documents'])}\")\n",
    "        print(f\"  ‚Ä¢ Images: {len(result['image_paths'])}\")\n",
    "        print(f\"  ‚Ä¢ Multimodal question: ‚úÖ\")\n",
    "        print(f\"  ‚Ä¢ Answer collected: ‚úÖ\")\n",
    "        print(f\"  ‚Ä¢ Multimodal feedback: ‚úÖ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        print(\"Please check your folder path and Groq API key.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_multimodal_qa_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b2011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
