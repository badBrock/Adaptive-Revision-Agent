{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba39d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def process_with_groq_clip(state):\n",
    "    # Get API key from environment\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    \n",
    "    # Debug: Check if API key is loaded\n",
    "    if not api_key:\n",
    "        print(\"‚ùå GROQ_API_KEY not found in environment variables\")\n",
    "        return {**state, \"llm_response\": \"API key not configured\"}\n",
    "    \n",
    "    print(f\"‚úÖ API key loaded: {api_key[:6]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e97411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Interactive Q&A with CLIP Embeddings\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading content from: E:\\7. Projects From Sem 3\\RAG\\data\\rag_1_Intro_20250906_032403\\images\n",
      "‚úÖ Found 0 documents and 2 images\n",
      "üé® Processing 2 images with CLIP...\n",
      "‚úÖ Pasted image 20250714150825.png\n",
      "‚úÖ Pasted image 20250714163303.png\n",
      "üîó Generated embeddings: 2 images, 0 texts\n",
      "\n",
      "Your turn! Please answer the question above:\n",
      "\n",
      "‚úÖ Answer recorded: 19 characters\n",
      "\n",
      "üí≠ FEEDBACK:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "It seems like you were expecting to find or provide some documents, but unfortunately, none were found. That's okay! It's an easy fix. \n",
      "\n",
      "To resolve this, you can try adding some markdown files to the specified folder. This should help you get started. If you're unsure about how to do this or need more guidance, feel free to ask, and I'll be happy to help. \n",
      "\n",
      "Keep going, and I'm sure you'll get everything sorted out!\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üéâ SESSION COMPLETE!\n",
      "üìä Summary:\n",
      "  ‚Ä¢ Documents: 0\n",
      "  ‚Ä¢ Images: 2\n",
      "  ‚Ä¢ Embeddings generated: ‚úÖ\n",
      "  ‚Ä¢ Question asked: ‚úÖ\n",
      "  ‚Ä¢ Answer collected: ‚úÖ\n",
      "  ‚Ä¢ Feedback provided: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Dict, Any, List\n",
    "import os\n",
    "import markdown\n",
    "import re\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your Groq API key here or use environment variable\n",
    "# # GROQ_API_KEY = \"gsk_your_actual_groq_api_key_here\"  # Replace with your key\n",
    "# os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "class QuestionState(TypedDict):\n",
    "    folder_path: str\n",
    "    documents: List[Dict[str, Any]]\n",
    "    image_paths: List[str]\n",
    "    image_embeddings: List[List[float]]\n",
    "    text_embeddings: List[List[float]]\n",
    "    question: str\n",
    "    user_answer: str\n",
    "    feedback: str\n",
    "\n",
    "def extract_image_paths(md_text: str, folder_path: str) -> List[str]:\n",
    "    img_pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    relative_paths = re.findall(img_pattern, md_text)\n",
    "    absolute_paths = []\n",
    "    \n",
    "    for img_path in relative_paths:\n",
    "        if not os.path.isabs(img_path):\n",
    "            abs_path = os.path.join(folder_path, img_path)\n",
    "            if os.path.exists(abs_path):\n",
    "                absolute_paths.append(abs_path)\n",
    "        else:\n",
    "            if os.path.exists(img_path):\n",
    "                absolute_paths.append(img_path)\n",
    "    \n",
    "    return absolute_paths\n",
    "\n",
    "def get_clip_embeddings(image_paths: List[str]) -> List[List[float]]:\n",
    "    if not image_paths:\n",
    "        return []\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"üé® Processing {len(image_paths)} images with CLIP...\")\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = model.encode_image(image)\n",
    "                embedding = embedding.cpu().numpy().flatten().tolist()\n",
    "            embeddings.append(embedding)\n",
    "            print(f\"‚úÖ {os.path.basename(img_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with {img_path}: {str(e)}\")\n",
    "            embeddings.append([0.0] * 512)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def get_text_clip_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"üìù Processing {len(texts)} texts with CLIP...\")\n",
    "    \n",
    "    for text in texts:\n",
    "        try:\n",
    "            text_tokens = clip.tokenize([text[:77]]).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = model.encode_text(text_tokens)\n",
    "                embedding = embedding.cpu().numpy().flatten().tolist()\n",
    "            embeddings.append(embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing text: {str(e)}\")\n",
    "            embeddings.append([0.0] * 512)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Node 1: Load documents and images\n",
    "def load_content(state: QuestionState) -> QuestionState:\n",
    "    folder_path = state[\"folder_path\"]\n",
    "    \n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"Invalid folder path: {folder_path}\")\n",
    "    \n",
    "    documents = []\n",
    "    all_image_paths = []\n",
    "    \n",
    "    print(f\"üìÅ Loading content from: {folder_path}\")\n",
    "    \n",
    "    # Load markdown files\n",
    "    for file_path in Path(folder_path).glob(\"*.md\"):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            md_content = f.read()\n",
    "        \n",
    "        image_paths = extract_image_paths(md_content, folder_path)\n",
    "        all_image_paths.extend(image_paths)\n",
    "        \n",
    "        documents.append({\n",
    "            \"filename\": file_path.name,\n",
    "            \"markdown\": md_content,\n",
    "            \"word_count\": len(md_content.split())\n",
    "        })\n",
    "    \n",
    "    # Find additional images\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}\n",
    "    for file_path in Path(folder_path).rglob(\"*\"):\n",
    "        if file_path.suffix.lower() in image_extensions:\n",
    "            abs_path = str(file_path.absolute())\n",
    "            if abs_path not in all_image_paths:\n",
    "                all_image_paths.append(abs_path)\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(documents)} documents and {len(all_image_paths)} images\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"documents\": documents,\n",
    "        \"image_paths\": all_image_paths\n",
    "    }\n",
    "\n",
    "# Node 2: Generate CLIP embeddings\n",
    "def generate_embeddings(state: QuestionState) -> QuestionState:\n",
    "    documents = state[\"documents\"]\n",
    "    image_paths = state[\"image_paths\"]\n",
    "    \n",
    "    image_embeddings = get_clip_embeddings(image_paths)\n",
    "    texts = [doc[\"markdown\"] for doc in documents]\n",
    "    text_embeddings = get_text_clip_embeddings(texts)\n",
    "    \n",
    "    print(f\"üîó Generated embeddings: {len(image_embeddings)} images, {len(text_embeddings)} texts\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"image_embeddings\": image_embeddings,\n",
    "        \"text_embeddings\": text_embeddings\n",
    "    }\n",
    "\n",
    "# Node 3: Generate question using Groq\n",
    "def generate_question(state: QuestionState) -> QuestionState:\n",
    "    documents = state[\"documents\"]\n",
    "    image_paths = state[\"image_paths\"]\n",
    "    \n",
    "    if not documents:\n",
    "        question = \"‚ùå No documents found. Please add markdown files to the folder.\"\n",
    "        return {**state, \"question\": question}\n",
    "    \n",
    "    # Combine content for question generation\n",
    "    combined_content = \"\\n\\n\".join([doc[\"markdown\"] for doc in documents])\n",
    "    \n",
    "    # Create prompt for Groq\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following content, generate ONE thoughtful question that tests understanding of the key concepts. \n",
    "    The question should be clear, specific, and require the person to demonstrate comprehension.\n",
    "    \n",
    "    Content ({len(documents)} documents, {len(image_paths)} images):\n",
    "    {combined_content[:2000]}...\n",
    "    \n",
    "    Generate only the question, nothing else.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        question = response.choices[0].message.content.strip()\n",
    "        print(f\"\\nü§î QUESTION GENERATED:\")\n",
    "        print(f\"‚îÄ\" * 50)\n",
    "        print(question)\n",
    "        print(f\"‚îÄ\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        question = f\"‚ùå Error generating question: {str(e)}\\n\\nFallback question: What are the main topics covered in the documents?\"\n",
    "        print(question)\n",
    "    \n",
    "    return {**state, \"question\": question}\n",
    "\n",
    "# Node 4: Collect user answer\n",
    "def collect_answer(state: QuestionState) -> QuestionState:\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"\\nYour turn! Please answer the question above:\")\n",
    "    user_answer = input(\"Your answer: \").strip()\n",
    "    \n",
    "    if not user_answer:\n",
    "        user_answer = \"No answer provided.\"\n",
    "    \n",
    "    print(f\"\\n‚úÖ Answer recorded: {len(user_answer)} characters\")\n",
    "    \n",
    "    return {**state, \"user_answer\": user_answer}\n",
    "\n",
    "# Node 5: Provide feedback\n",
    "def provide_feedback(state: QuestionState) -> QuestionState:\n",
    "    question = state[\"question\"]\n",
    "    user_answer = state[\"user_answer\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Create context for feedback\n",
    "    content_summary = \"\\n\".join([doc[\"markdown\"][:300] for doc in documents])\n",
    "    \n",
    "    feedback_prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    \n",
    "    User's Answer: {user_answer}\n",
    "    \n",
    "    Content Context: {content_summary}\n",
    "    \n",
    "    Provide brief, constructive feedback on the user's answer. Be encouraging but honest about accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": feedback_prompt}],\n",
    "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "            max_tokens=300,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        feedback = response.choices[0].message.content.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        feedback = f\"Thank you for your answer! Due to a technical issue, I can't provide detailed feedback right now, but your response has been recorded.\"\n",
    "    \n",
    "    print(f\"\\nüí≠ FEEDBACK:\")\n",
    "    print(f\"‚îÄ\" * 50)\n",
    "    print(feedback)\n",
    "    print(f\"‚îÄ\" * 50)\n",
    "    \n",
    "    return {**state, \"feedback\": feedback}\n",
    "\n",
    "# Build the workflow\n",
    "def create_qa_workflow():\n",
    "    workflow = StateGraph(QuestionState)\n",
    "    \n",
    "    workflow.add_node(\"load_content\", load_content)\n",
    "    workflow.add_node(\"generate_embeddings\", generate_embeddings)\n",
    "    workflow.add_node(\"generate_question\", generate_question)\n",
    "    workflow.add_node(\"collect_answer\", collect_answer)\n",
    "    workflow.add_node(\"provide_feedback\", provide_feedback)\n",
    "    \n",
    "    workflow.set_entry_point(\"load_content\")\n",
    "    workflow.add_edge(\"load_content\", \"generate_embeddings\")\n",
    "    workflow.add_edge(\"generate_embeddings\", \"generate_question\")\n",
    "    workflow.add_edge(\"generate_question\", \"collect_answer\")\n",
    "    workflow.add_edge(\"collect_answer\", \"provide_feedback\")\n",
    "    workflow.add_edge(\"provide_feedback\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Main execution\n",
    "def run_qa_session():\n",
    "    print(\"üéØ Interactive Q&A with CLIP Embeddings\")\n",
    "    print(\"‚ïê\" * 40)\n",
    "    \n",
    "    # Get folder path\n",
    "    folder_path = input(\"Enter folder path with markdown files and images: \").strip()\n",
    "    if not folder_path:\n",
    "        folder_path = \".\"\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = QuestionState(\n",
    "        folder_path=folder_path,\n",
    "        documents=[],\n",
    "        image_paths=[],\n",
    "        image_embeddings=[],\n",
    "        text_embeddings=[],\n",
    "        question=\"\",\n",
    "        user_answer=\"\",\n",
    "        feedback=\"\"\n",
    "    )\n",
    "    \n",
    "    # Run workflow\n",
    "    try:\n",
    "        workflow = create_qa_workflow()\n",
    "        result = workflow.invoke(initial_state)\n",
    "        \n",
    "        print(f\"\\nüéâ SESSION COMPLETE!\")\n",
    "        print(f\"üìä Summary:\")\n",
    "        print(f\"  ‚Ä¢ Documents: {len(result['documents'])}\")\n",
    "        print(f\"  ‚Ä¢ Images: {len(result['image_paths'])}\")\n",
    "        print(f\"  ‚Ä¢ Embeddings generated: ‚úÖ\")\n",
    "        print(f\"  ‚Ä¢ Question asked: ‚úÖ\")\n",
    "        print(f\"  ‚Ä¢ Answer collected: ‚úÖ\")\n",
    "        print(f\"  ‚Ä¢ Feedback provided: ‚úÖ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        print(\"Please check your folder path and Groq API key.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_qa_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea554699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
